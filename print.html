<!DOCTYPE HTML>
<html lang="zh-CN" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Ktransformers</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Ktransformers</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/kvcache-ai/ktransformers" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div align="center">
  <!-- <h1 id="ktransformers"><a class="header" href="#ktransformers">KTransformers</a></h1> -->
  <p align="center">
<picture>
    <img alt="KTransformers" src="https://github.com/user-attachments/assets/d5a2492f-a415-4456-af99-4ab102f13f8b" width=50%>
</picture>
</p>
</div>
<h2 id="intro"><a class="header" href="#intro">🎉 Introduction</a></h2>
KTransformers, pronounced as Quick Transformers, is designed to enhance your 🤗 <a href="https://github.com/huggingface/transformers">Transformers</a> experience with advanced kernel optimizations and placement/parallelism strategies.
<br/><br/>
KTransformers is a flexible, Python-centric framework designed with extensibility at its core. 
By implementing and injecting an optimized module with a single line of code, users gain access to a Transformers-compatible
interface, RESTful APIs compliant with OpenAI and Ollama, and even a simplified ChatGPT-like web UI. 
<br/><br/>
Our vision for KTransformers is to serve as a flexible platform for experimenting with innovative LLM inference optimizations. Please let us know if you need any other features.
<h2 id="Updates"><a class="header" href="#Updates">🔥 Updates</a></h2>
<ul>
<li><strong>Mar 15, 2025</strong>: Support ROCm on AMD GPU (<a href="./en/ROCm.html">Tutorial</a>).</li>
<li><strong>Mar 5, 2025</strong>: Support unsloth 1.58/2.51 bits weights and <a href="./en/fp8_kernel.html">IQ1_S/FP8 hybrid</a> weights. Support 139K <a href="./en/DeepseekR1_V3_tutorial.html#v022-longer-context">Longer Context</a> for DeepSeek-V3 and R1 in 24GB VRAM.</li>
<li><strong>Feb 25, 2025</strong>: Support <a href="./en/fp8_kernel.html">FP8 GPU kernel</a> for DeepSeek-V3 and R1; <a href="./en/DeepseekR1_V3_tutorial.html#v022-longer-context">Longer Context</a>.</li>
<li><strong>Feb 10, 2025</strong>: Support Deepseek-R1 and V3 on single (24GB VRAM)/multi gpu and 382G DRAM, up to 3~28x speedup. The detailed tutorial is <a href="./en/DeepseekR1_V3_tutorial.html">here</a>.</li>
<li><strong>Aug 28, 2024</strong>: Support 1M context under the InternLM2.5-7B-Chat-1M model, utilizing 24GB of VRAM and 150GB of DRAM. The detailed tutorial is <a href="./en/long_context_tutorial.html">here</a>.</li>
<li><strong>Aug 28, 2024</strong>: Decrease DeepseekV2's required VRAM from 21G to 11G.</li>
<li><strong>Aug 15, 2024</strong>: Update detailed <a href="./en/injection_tutorial.html">TUTORIAL</a> for injection and multi-GPU.</li>
<li><strong>Aug 14, 2024</strong>: Support llamfile as linear backend.</li>
<li><strong>Aug 12, 2024</strong>: Support multiple GPU; Support new model: mixtral 8*7B  and 8*22B; Support q2k, q3k, q5k dequant on gpu.</li>
<li><strong>Aug 9, 2024</strong>: Support windows native.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><!-- omit in toc -->
<h1 id="how-to-run-deepseek-r1"><a class="header" href="#how-to-run-deepseek-r1">How to Run DeepSeek-R1</a></h1>
<ul>
<li><a href="en/install.html#preparation">Preparation</a></li>
<li><a href="en/install.html#installation">Installation</a>
<ul>
<li><a href="en/install.html#attention">Attention</a></li>
<li><a href="en/install.html#supported-models-include">Supported models include:</a></li>
<li><a href="en/install.html#support-quantize-format">Support quantize format:</a></li>
</ul>
</li>
</ul>
<p>In this document, we will show you how to install and run KTransformers on your local machine. There are two versions:</p>
<ul>
<li>V0.2 is the current main branch.</li>
<li>V0.3 is a preview version only provides binary distribution for now.</li>
<li>To reproduce our DeepSeek-R1/V3 results, please refer to <a href="en/./DeepseekR1_V3_tutorial.html">Deepseek-R1/V3 Tutorial</a> for more detail settings after installation.</li>
</ul>
<h2 id="preparation"><a class="header" href="#preparation">Preparation</a></h2>
<p>Some preparation:</p>
<ul>
<li>
<p>CUDA 12.1 and above, if you didn't have it yet, you may install from <a href="https://developer.nvidia.com/cuda-downloads">here</a>.</p>
<pre><code class="language-sh"># Adding CUDA to PATH
if [ -d "/usr/local/cuda/bin" ]; then
    export PATH=$PATH:/usr/local/cuda/bin
fi

if [ -d "/usr/local/cuda/lib64" ]; then
    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64
    # Or you can add it to /etc/ld.so.conf and run ldconfig as root:
    # echo "/usr/local/cuda-12.x/lib64" | sudo tee -a /etc/ld.so.conf
    # sudo ldconfig
fi

if [ -d "/usr/local/cuda" ]; then
    export CUDA_PATH=$CUDA_PATH:/usr/local/cuda
fi
</code></pre>
</li>
<li>
<p>Linux-x86_64 with gcc, g++ and cmake (using Ubuntu as an example)</p>
<pre><code class="language-sh">sudo apt-get update
sudo apt-get install build-essential cmake ninja-build
</code></pre>
</li>
<li>
<p>We recommend using <a href="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh">Miniconda3</a> or <a href="https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh">Anaconda3</a> to create a virtual environment with Python=3.11 to run our program. Assuming your Anaconda installation directory is <code>~/anaconda3</code>, you should ensure that the version identifier of the GNU C++standard library used by Anaconda includes <code>GLIBCXX-3.4.32</code></p>
<pre><code class="language-sh">conda create --name ktransformers python=3.11
conda activate ktransformers # you may need to run ‘conda init’ and reopen shell first

conda install -c conda-forge libstdcxx-ng # Anaconda provides a package called `libstdcxx-ng` that includes a newer version of `libstdc++`, which can be installed via `conda-forge`.

strings ~/anaconda3/envs/ktransformers/lib/libstdc++.so.6 | grep GLIBCXX
</code></pre>
</li>
<li>
<p>Make sure that PyTorch, packaging, ninja is installed You can also <a href="https://pytorch.org/get-started/previous-versions/">install previous versions of PyTorch</a></p>
<pre><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
pip3 install packaging ninja cpufeature numpy
</code></pre>
</li>
<li>
<p>At the same time, you should download and install the corresponding version of flash-attention from https://github.com/Dao-AILab/flash-attention/releases.</p>
</li>
</ul>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<h3 id="attention"><a class="header" href="#attention">Attention</a></h3>
<p>If you want to use numa support, not only do you need to set USE_NUMA=1, but you also need to make sure you have installed the libnuma-dev (<code>sudo apt-get install libnuma-dev</code> may help you).</p>
<!-- 1. ~~Use a Docker image, see [documentation for Docker](./doc/en/Docker.md)~~
   
   >We are working on the latest docker image, please wait for a while.

2. ~~You can install using Pypi (for linux):~~
    > We are working on the latest pypi package, please wait for a while.
   
   ```
   pip install ktransformers --no-build-isolation
   ```
   
   for windows we prepare a pre compiled whl package on [ktransformers-0.2.0+cu125torch24avx2-cp312-cp312-win_amd64.whl](https://github.com/kvcache-ai/ktransformers/releases/download/v0.2.0/ktransformers-0.2.0+cu125torch24avx2-cp312-cp312-win_amd64.whl), which require cuda-12.5, torch-2.4, python-3.11, more pre compiled package are being produced.  -->
<ul>
<li>
<p>Download source code and compile:</p>
<ul>
<li>
<p>init source code</p>
<pre><code class="language-sh">git clone https://github.com/kvcache-ai/ktransformers.git
cd ktransformers
git submodule init
git submodule update
</code></pre>
</li>
<li>
<p>[Optional] If you want to run with website, please <a href="en/./api/server/website.html">compile the website</a> before execute <code>bash install.sh</code></p>
</li>
<li>
<p>For Linux</p>
<ul>
<li>
<p>For simple install:</p>
<pre><code class="language-shell">bash install.sh
</code></pre>
</li>
<li>
<p>For those who have two cpu and 1T RAM:</p>
<pre><code class="language-shell"> # Make sure your system has dual sockets and double size RAM than the model's size (e.g. 1T RAM for 512G model)
 apt install libnuma-dev
 export USE_NUMA=1
 bash install.sh # or #make dev_install
</code></pre>
</li>
</ul>
</li>
<li>
<p>For Windows</p>
<pre><code class="language-shell">install.bat
</code></pre>
</li>
</ul>
</li>
<li>
<p>If you are developer, you can make use of the makefile to compile and format the code. <br> the detailed usage of makefile is <a href="en/./makefile_usage.html">here</a></p>
</li>
</ul>
<h3 id="local-chat"><a class="header" href="#local-chat">Local Chat</a></h3>
We provide a simple command-line local chat Python script that you can run for testing.
<blockquote>
<p>Note: this is a very simple test tool only support one round chat without any memory about last input, if you want to try full ability of the model, you may go to <a href="en/install.html#id_666">RESTful API and Web UI</a>.</p>
</blockquote>
<h4 id="run-example"><a class="header" href="#run-example">Run Example</a></h4>
<pre><code class="language-shell"># Begin from root of your cloned repo!
# Begin from root of your cloned repo!!
# Begin from root of your cloned repo!!! 

# Download mzwing/DeepSeek-V2-Lite-Chat-GGUF from huggingface
mkdir DeepSeek-V2-Lite-Chat-GGUF
cd DeepSeek-V2-Lite-Chat-GGUF

wget https://huggingface.co/mradermacher/DeepSeek-V2-Lite-GGUF/resolve/main/DeepSeek-V2-Lite.Q4_K_M.gguf -O DeepSeek-V2-Lite-Chat.Q4_K_M.gguf

cd .. # Move to repo's root dir

# Start local chat
python -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF

# If you see “OSError: We couldn't connect to 'https://huggingface.co' to load this file”, try：
# GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite
# python  ktransformers.local_chat --model_path ./DeepSeek-V2-Lite --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF
</code></pre>
<p>It features the following arguments:</p>
<ul>
<li>
<p><code>--model_path</code> (required): Name of the model (such as "deepseek-ai/DeepSeek-V2-Lite-Chat" which will automatically download configs from <a href="https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite">Hugging Face</a>). Or if you already got local files  you may directly use that path to initialize the model.</p>
<blockquote>
<p>Note: <strong>.safetensors</strong> files are not required in the directory. We only need config files to build model and tokenizer.</p>
</blockquote>
</li>
<li>
<p><code>--gguf_path</code> (required): Path of a directory containing GGUF files which could that can be downloaded from <a href="https://huggingface.co/mzwing/DeepSeek-V2-Lite-Chat-GGUF/tree/main">Hugging Face</a>. Note that the directory should only contains GGUF of current model, which means you need one separate directory for each model.</p>
</li>
<li>
<p><code>--optimize_config_path</code> (required except for Qwen2Moe and DeepSeek-V2): Path of YAML file containing optimize rules. There are two rule files pre-written in the <a href="en/ktransformers/optimize/optimize_rules">ktransformers/optimize/optimize_rules</a> directory for optimizing DeepSeek-V2 and Qwen2-57B-A14, two SOTA MoE models.</p>
</li>
<li>
<p><code>--max_new_tokens</code>: Int (default=1000). Maximum number of new tokens to generate.</p>
</li>
<li>
<p><code>--cpu_infer</code>: Int (default=10). The number of CPUs used for inference. Should ideally be set to the (total number of cores - 2).</p>
</li>
</ul>
<details>
<summary>Supported Models/quantization</summary>
<h3 id="supported-models-include"><a class="header" href="#supported-models-include">Supported models include:</a></h3>
<div class="table-wrapper"><table><thead><tr><th>✅ <strong>Supported Models</strong></th><th>❌ <strong>Deprecated Models</strong></th></tr></thead><tbody>
<tr><td>DeepSeek-R1</td><td><del>InternLM2.5-7B-Chat-1M</del></td></tr>
<tr><td>DeepSeek-V3</td><td></td></tr>
<tr><td>DeepSeek-V2</td><td></td></tr>
<tr><td>DeepSeek-V2.5</td><td></td></tr>
<tr><td>Qwen2-57B</td><td></td></tr>
<tr><td>DeepSeek-V2-Lite</td><td></td></tr>
<tr><td>Mixtral-8x7B</td><td></td></tr>
<tr><td>Mixtral-8x22B</td><td></td></tr>
</tbody></table>
</div>
<h3 id="support-quantize-format"><a class="header" href="#support-quantize-format">Support quantize format:</a></h3>
<div class="table-wrapper"><table><thead><tr><th>✅ <strong>Supported Formats</strong></th><th>❌ <strong>Deprecated Formats</strong></th></tr></thead><tbody>
<tr><td>Q2_K_L</td><td><del>IQ2_XXS</del></td></tr>
<tr><td>Q2_K_XS</td><td></td></tr>
<tr><td>Q3_K_M</td><td></td></tr>
<tr><td>Q4_K_M</td><td></td></tr>
<tr><td>Q5_K_M</td><td></td></tr>
<tr><td>Q6_K</td><td></td></tr>
<tr><td>Q8_0</td><td></td></tr>
</tbody></table>
</div></details>
<details>
<summary>Suggested Model</summary>
<div class="table-wrapper"><table><thead><tr><th>Model Name</th><th>Model Size</th><th>VRAM</th><th>Minimum DRAM</th><th>Recommended DRAM</th></tr></thead><tbody>
<tr><td>DeepSeek-R1-q4_k_m</td><td>377G</td><td>14G</td><td>382G</td><td>512G</td></tr>
<tr><td>DeepSeek-V3-q4_k_m</td><td>377G</td><td>14G</td><td>382G</td><td>512G</td></tr>
<tr><td>DeepSeek-V2-q4_k_m</td><td>133G</td><td>11G</td><td>136G</td><td>192G</td></tr>
<tr><td>DeepSeek-V2.5-q4_k_m</td><td>133G</td><td>11G</td><td>136G</td><td>192G</td></tr>
<tr><td>DeepSeek-V2.5-IQ4_XS</td><td>117G</td><td>10G</td><td>107G</td><td>128G</td></tr>
<tr><td>Qwen2-57B-A14B-Instruct-q4_k_m</td><td>33G</td><td>8G</td><td>34G</td><td>64G</td></tr>
<tr><td>DeepSeek-V2-Lite-q4_k_m</td><td>9.7G</td><td>3G</td><td>13G</td><td>16G</td></tr>
<tr><td>Mixtral-8x7B-q4_k_m</td><td>25G</td><td>1.6G</td><td>51G</td><td>64G</td></tr>
<tr><td>Mixtral-8x22B-q4_k_m</td><td>80G</td><td>4G</td><td>86.1G</td><td>96G</td></tr>
<tr><td>InternLM2.5-7B-Chat-1M</td><td>15.5G</td><td>15.5G</td><td>8G(32K context)</td><td>150G (1M context)</td></tr>
</tbody></table>
</div>
<p>More will come soon. Please let us know which models you are most interested in.</p>
<p>Be aware that you need to be subject to their corresponding model licenses when using <a href="https://huggingface.co/deepseek-ai/DeepSeek-V2/blob/main/LICENSE">DeepSeek</a> and <a href="https://huggingface.co/Qwen/Qwen2-72B-Instruct/blob/main/LICENSE">QWen</a>.</p>
</details>
<details>
  <summary>Click To Show how to run other examples</summary>
<ul>
<li>
<p>Qwen2-57B</p>
<pre><code class="language-sh">pip install flash_attn # For Qwen2

mkdir Qwen2-57B-GGUF &amp;&amp; cd Qwen2-57B-GGUF

wget https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct-GGUF/resolve/main/qwen2-57b-a14b-instruct-q4_k_m.gguf?download=true -O qwen2-57b-a14b-instruct-q4_k_m.gguf

cd ..

python -m ktransformers.local_chat --model_name Qwen/Qwen2-57B-A14B-Instruct --gguf_path ./Qwen2-57B-GGUF

# If you see “OSError: We couldn't connect to 'https://huggingface.co' to load this file”, try：
# GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct
# python  ktransformers/local_chat.py --model_path ./Qwen2-57B-A14B-Instruct --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF
</code></pre>
</li>
<li>
<p>Deepseek-V2</p>
<pre><code class="language-sh">mkdir DeepSeek-V2-Chat-0628-GGUF &amp;&amp; cd DeepSeek-V2-Chat-0628-GGUF
# Download weights
wget https://huggingface.co/bartowski/DeepSeek-V2-Chat-0628-GGUF/resolve/main/DeepSeek-V2-Chat-0628-Q4_K_M/DeepSeek-V2-Chat-0628-Q4_K_M-00001-of-00004.gguf -o DeepSeek-V2-Chat-0628-Q4_K_M-00001-of-00004.gguf
wget https://huggingface.co/bartowski/DeepSeek-V2-Chat-0628-GGUF/resolve/main/DeepSeek-V2-Chat-0628-Q4_K_M/DeepSeek-V2-Chat-0628-Q4_K_M-00002-of-00004.gguf -o DeepSeek-V2-Chat-0628-Q4_K_M-00002-of-00004.gguf
wget https://huggingface.co/bartowski/DeepSeek-V2-Chat-0628-GGUF/resolve/main/DeepSeek-V2-Chat-0628-Q4_K_M/DeepSeek-V2-Chat-0628-Q4_K_M-00003-of-00004.gguf -o DeepSeek-V2-Chat-0628-Q4_K_M-00003-of-00004.gguf
wget https://huggingface.co/bartowski/DeepSeek-V2-Chat-0628-GGUF/resolve/main/DeepSeek-V2-Chat-0628-Q4_K_M/DeepSeek-V2-Chat-0628-Q4_K_M-00004-of-00004.gguf -o DeepSeek-V2-Chat-0628-Q4_K_M-00004-of-00004.gguf

cd ..

python -m ktransformers.local_chat --model_name deepseek-ai/DeepSeek-V2-Chat-0628 --gguf_path ./DeepSeek-V2-Chat-0628-GGUF

# If you see “OSError: We couldn't connect to 'https://huggingface.co' to load this file”, try：

# GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat-0628

# python -m ktransformers.local_chat --model_path ./DeepSeek-V2-Chat-0628 --gguf_path ./DeepSeek-V2-Chat-0628-GGUF
</code></pre>
</li>
</ul>
<div class="table-wrapper"><table><thead><tr><th>model name</th><th>weights download link</th></tr></thead><tbody>
<tr><td>Qwen2-57B</td><td><a href="https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct-GGUF/tree/main">Qwen2-57B-A14B-gguf-Q4K-M</a></td></tr>
<tr><td>DeepseekV2-coder</td><td><a href="https://huggingface.co/LoneStriker/DeepSeek-Coder-V2-Instruct-GGUF/tree/main">DeepSeek-Coder-V2-Instruct-gguf-Q4K-M</a></td></tr>
<tr><td>DeepseekV2-chat</td><td><a href="https://huggingface.co/bullerwins/DeepSeek-V2-Chat-0628-GGUF/tree/main">DeepSeek-V2-Chat-gguf-Q4K-M</a></td></tr>
<tr><td>DeepseekV2-lite</td><td><a href="https://huggingface.co/mzwing/DeepSeek-V2-Lite-Chat-GGUF/tree/main">DeepSeek-V2-Lite-Chat-GGUF-Q4K-M</a></td></tr>
<tr><td>DeepSeek-R1</td><td><a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-Q4_K_M">DeepSeek-R1-gguf-Q4K-M</a></td></tr>
</tbody></table>
</div></details>
<!-- pin block for jump -->
<span id='id_666'> 
<h3 id="restful-api-and-web-ui"><a class="header" href="#restful-api-and-web-ui">RESTful API and Web UI  </a></h3>
<p>Start without website:</p>
<pre><code class="language-sh">ktransformers --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path /path/to/DeepSeek-V2-Lite-Chat-GGUF --port 10002
</code></pre>
<p>Start with website:</p>
<pre><code class="language-sh">ktransformers --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path /path/to/DeepSeek-V2-Lite-Chat-GGUF  --port 10002 --web True
</code></pre>
<p>Or you want to start server with transformers, the model_path should include safetensors</p>
<pre><code class="language-bash">ktransformers --type transformers --model_path /mnt/data/model/Qwen2-0.5B-Instruct --port 10002 --web True
</code></pre>
<p>Access website with url <a href="http://localhost:10002/web/index.html#/chat">http://localhost:10002/web/index.html#/chat</a> :</p>
<p align="center">
  <picture>
    <img alt="Web UI" src="https://github.com/user-attachments/assets/615dca9b-a08c-4183-bbd3-ad1362680faf" width=90%>
  </picture>
</p>
<p>More information about the RESTful API server can be found <a href="en/doc/en/api/server/server.html">here</a>. You can also find an example of integrating with Tabby <a href="en/doc/en/api/server/tabby.html">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- omit in toc -->
<h1 id="gpt-4o1-level-local-vscode-copilot-on-a-desktop-with-only-24gb-vram"><a class="header" href="#gpt-4o1-level-local-vscode-copilot-on-a-desktop-with-only-24gb-vram">GPT-4/o1-level Local VSCode Copilot on a Desktop with only 24GB VRAM</a></h1>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#summary">SUMMARY</a>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#show-case-environment">Show Case Environment</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#bench-result">Bench Result</a>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#v021">V0.2.1</a>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#memory-consumption">Memory consumption:</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#change-log">Change Log</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#benchmark-results">Benchmark Results</a></li>
</ul>
</li>
<li><a href="en/DeepseekR1_V3_tutorial.html#v02">V0.2</a>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#settings">Settings</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#memory-consumption-1">Memory consumption:</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#benchmark-results-1">Benchmark Results</a></li>
</ul>
</li>
<li><a href="en/DeepseekR1_V3_tutorial.html#v03-preview">V0.3-Preview</a>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#settings-1">Settings</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#memory-consumptions">Memory consumptions:</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#benchmark-results-2">Benchmark results</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="en/DeepseekR1_V3_tutorial.html#how-to-run">How to Run</a>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#v022--v023-longer-context--fp8-kernel">v0.2.2 &amp; v0.2.3 longer context &amp; FP8 kernel</a>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#longer-context">longer context</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#fp8-kernel">FP8 kernel</a></li>
</ul>
</li>
<li><a href="en/DeepseekR1_V3_tutorial.html#v02--v021-showcase">V0.2 &amp; V0.2.1 Showcase</a>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#single-socket-version-32-cores">Single socket version (32 cores)</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#dual-socket-version-64-cores">Dual socket version (64 cores)</a></li>
</ul>
</li>
<li><a href="en/DeepseekR1_V3_tutorial.html#v03-showcase">V0.3 Showcase</a>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#dual-socket-version-64-cores-1">Dual socket version (64 cores)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="en/DeepseekR1_V3_tutorial.html#some-explanations">Some Explanations</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#next">Next</a>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#faster">Faster</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#easier">Easier</a></li>
</ul>
</li>
<li><a href="en/DeepseekR1_V3_tutorial.html#faq">FAQ</a>
<ul>
<li><a href="en/DeepseekR1_V3_tutorial.html#r1-no-thinking">R1 No Thinking</a></li>
<li><a href="en/DeepseekR1_V3_tutorial.html#more-faq">More FAQ</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="summary"><a class="header" href="#summary">SUMMARY</a></h1>
<blockquote>
<p><strong>Feb 10, 2025</strong>: Support DeepseekR1 and V3 on single (24GB VRAM)/multi gpu and 382G DRAM, up to 3~28x speedup.<br></p>
</blockquote>
<p>Hi, we're the KTransformers team (formerly known for our local CPU/GPU hybrid inference open source project with DeepSeek-V2).</p>
<p>We've heard your requests for DeepSeek-R1/V3 support—and we're excited to finally deliver!
Apologies for the wait, but we've been cooking up something truly amazing!</p>
<p>Today, we're proud to announce that we not only support DeepSeek-R1/V3, as showcased in the video below:</p>
<p>https://github.com/user-attachments/assets/ebd70bfa-b2c1-4abb-ae3b-296ed38aa285</p>
</p>
<ul>
<li><strong>[NEW!!!] Local 671B DeepSeek-Coder-V3/R1:</strong> Running its Q4_K_M version using only 14GB VRAM and 382GB DRAM.
<ul>
<li>Prefill Speed (tokens/s):
<ul>
<li>KTransformers: 54.21 (32 cores) → 74.362 (dual-socket, 2×32 cores) → 255.26 (optimized AMX-based MoE kernel, V0.3 only) → 286.55 (selectively using 6 experts, V0.3 only)</li>
<li>Compared to 10.31 tokens/s in llama.cpp with 2×32 cores, achieving up to <strong>27.79× speedup</strong>.</li>
</ul>
</li>
<li>Decode Speed (tokens/s):
<ul>
<li>KTransformers: 8.73 (32 cores) → 11.26 (dual-socket, 2×32 cores) → 13.69 (selectively using 6 experts, V0.3 only)</li>
<li>Compared to 4.51 tokens/s in llama.cpp with 2×32 cores, achieving up to <strong>3.03× speedup</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>We also give our upcoming optimizations previews, including an Intel AMX-accelerated kernel and a selective expert activation method, which will significantly enhance performance. With V0.3-preview, we achieve up to 286 tokens/s for prefill, making it up to <strong>28× faster than llama.cpp</strong> for local inference.
The binary distribution is available now and the source code will come ASAP! Check out the wheel package <a href="https://github.com/kvcache-ai/ktransformers/releases/download/v0.1.4/ktransformers-0.3.0rc0+cu126torch26fancy-cp311-cp311-linux_x86_64.whl">here</a></p>
<blockquote>
<p><strong>Feb 15, 2025</strong>: KTransformers V0.2.1: Longer Context (from 4K to 8K for 24GB VRAM) &amp; Slightly Faster Speed （+15%) (Up to 16 Tokens/s), update docs <a href="en/./doc/en/DeepseekR1_V3_tutorial.html">here</a> and <a href="https://kvcache-ai.github.io/ktransformers/">online books</a>.</p>
</blockquote>
<p>We speed up the decode and prefill speed a littlt bit. The reason for the limited performance improvement mainly lies in the fact that the inference process is still constrained by the CPU's computational speed and memory bandwidth. The MLA part handled by the GPU accounts for a relatively small proportion.</p>
<p>Besides the improvements in speed, we've also significantly updated the documentation to enhance usability, including:<br></p>
<ul>
<li>Added Multi-GPU configuration tutorial.</li>
<li>Consolidated installation guide.</li>
<li>Add a detailed tutorial on registering extra GPU memory with ExpertMarlin;</li>
</ul>
<h2 id="show-case-environment"><a class="header" href="#show-case-environment">Show Case Environment</a></h2>
<p>We run our best performance tests (V0.2) on <br>
CPU: Intel (R) Xeon (R) Gold 6454S 1T DRAM (2 NUMA nodes) <br>
GPU: 4090D 24G VRAM <br>
Memory: standard DDR5-4800 server DRAM (1 TB), each socket with 8×DDR5-4800</p>
<h2 id="bench-result"><a class="header" href="#bench-result">Bench Result</a></h2>
<h3 id="v021"><a class="header" href="#v021">V0.2.1</a></h3>
<ul>
<li>Model: DeepseekV3-q4km (int4)<br></li>
<li>CPU: cpu_model_name: Intel (R) Xeon (R) Gold 6454S, 32 cores per socket, 2 sockets, 2 numa nodes</li>
<li>GPU: 4090 24G VRAM</li>
<li>We test after enough warm up</li>
</ul>
<h4 id="memory-consumption"><a class="header" href="#memory-consumption">Memory consumption:</a></h4>
<ul>
<li>Single socket: 382G DRAM, at least 14GB VRAM</li>
<li>Dual socket: 1T DRAM, at least 14GB VRAM</li>
</ul>
<h4 id="change-log"><a class="header" href="#change-log">Change Log</a></h4>
<ul>
<li>Longer Context (from 4K to 8K for 24GB VRAM) and Slightly Faster Speed （+15%):<br>
Integrated the highly efficient Triton MLA Kernel from the fantastic sglang project, enable much longer context length and slightly faster prefill/decode speed</li>
<li>We suspect that some of the improvements come from the change of hardware platform (4090D-&gt;4090)</li>
</ul>
<h4 id="benchmark-results"><a class="header" href="#benchmark-results">Benchmark Results</a></h4>
<p>"6 experts" case is part of V0.3's preview</p>
<div class="table-wrapper"><table><thead><tr><th>Prompt</th><th>hi (2)</th><th>1K (969)</th><th>2K (1930)</th><th>4K (3846)</th><th>8K (7678)</th></tr></thead><tbody>
<tr><td>Output length</td><td>10tokens</td><td>300tokens</td><td>300tokens</td><td>300tokens</td><td>300tokens</td></tr>
<tr><td><strong>6 experts V0.2.0</strong></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Prefill token/s</td><td>13</td><td>105</td><td>102</td><td>88</td><td>CUDA OOM</td></tr>
<tr><td>decode token/s</td><td>16.8</td><td>15.4</td><td>14.2</td><td>13.0</td><td>CUDA OOM</td></tr>
<tr><td><strong>6 experts V0.2.1</strong></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Prefill token/s</td><td>13</td><td>111</td><td>112.5</td><td>102 <strong>(1.16x speedup)</strong></td><td>101</td></tr>
<tr><td>decode token/s</td><td>16.8</td><td>15.9</td><td>15.4</td><td>14.9 <strong>(1.15x speedup)</strong></td><td>13.9</td></tr>
<tr><td><strong>8 experts V0.2.1</strong></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Prefill token/s</td><td>12.2</td><td>88.2</td><td>88.5</td><td>81.9</td><td>80</td></tr>
<tr><td>Decode token/s</td><td>13.4</td><td>13.5</td><td>13.4</td><td>13.2</td><td>12.4</td></tr>
</tbody></table>
</div>
<h3 id="v02"><a class="header" href="#v02">V0.2</a></h3>
<h4 id="settings"><a class="header" href="#settings">Settings</a></h4>
<ul>
<li>Model: DeepseekV3-q4km (int4)<br></li>
<li>CPU: cpu_model_name: Intel (R) Xeon (R) Gold 6454S, 32 cores per socket, 2 sockets, 2 numa nodes</li>
<li>GPU: 4090D 24G VRAM</li>
<li>We test after enough warm up</li>
</ul>
<h4 id="memory-consumption-1"><a class="header" href="#memory-consumption-1">Memory consumption:</a></h4>
<ul>
<li>Single socket: 382G DRAM, at least 14GB VRAM</li>
<li>Dual socket: 1T DRAM, at least 14GB VRAM</li>
</ul>
<h4 id="benchmark-results-1"><a class="header" href="#benchmark-results-1">Benchmark Results</a></h4>
<p>"6 experts" case is part of V0.3's preview</p>
<div class="table-wrapper"><table><thead><tr><th>Prompt<br>(500 tokens)</th><th>Dual socket Ktrans (6 experts)</th><th>Dual socket Ktrans (8 experts)</th><th>Single socket Ktrans (6 experts)</th><th>Single socket Ktrans (8 experts)</th><th>llama.cpp (8 experts)</th></tr></thead><tbody>
<tr><td>Prefill token/s</td><td>97.32</td><td>82.94</td><td>65.14</td><td>54.21</td><td>10.31</td></tr>
<tr><td>Decode token/s</td><td>13.69</td><td>12.208</td><td>10.303</td><td>8.73</td><td>4.51</td></tr>
</tbody></table>
</div>
<p><strong>The highest speedup reaches up to <u>3.03x</u> in decoding and <u>9.44x</u> in prefill.</strong></p>
<h3 id="v03-preview"><a class="header" href="#v03-preview">V0.3-Preview</a></h3>
<h4 id="settings-1"><a class="header" href="#settings-1">Settings</a></h4>
<ul>
<li>Model: DeepseekV3-BF16 (online quant into int8 for CPU and int4 for GPU)</li>
<li>CPU: cpu_model_name: Intel (R) Xeon (R) Gold 6454S, 32 cores per socket, 2 socket, 2 numa nodes</li>
<li>GPU: (1~4)x 4090D 24GVRAM (requires more VRAM for longer prompt)</li>
</ul>
<h4 id="memory-consumptions"><a class="header" href="#memory-consumptions">Memory consumptions:</a></h4>
<ul>
<li>644GB DRAM, at least 14GB VRAM</li>
</ul>
<h4 id="benchmark-results-2"><a class="header" href="#benchmark-results-2">Benchmark results</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Prompt length</th><th>1K</th><th>2K</th><th>4K</th><th>8K</th></tr></thead><tbody>
<tr><td>KTrans (8 experts) Prefill token/s</td><td>185.96</td><td>255.26</td><td>252.58</td><td>195.62</td></tr>
<tr><td>KTrans (6 experts) Prefill token/s</td><td>203.70</td><td>286.55</td><td>271.08</td><td>207.20</td></tr>
</tbody></table>
</div>
<p><strong>The prefill of KTrans V0.3 is up to <u>3.45x</u> times faster than KTrans V0.2, and is up to <u>27.79x</u> times faster than llama.cpp.</strong>
<strong>The decoding speed is the same as KTrans V0.2 (6 experts version) so it is omitted</strong></p>
<p>The main acceleration comes from</p>
<ul>
<li>Intel AMX instruction set and our specially designed cache friendly memory layout</li>
<li>Expert selection strategy that selects fewer experts based on offline profile results of out of domain data</li>
</ul>
<p><em>From our research on DeepSeekV2, DeepSeekV3 and DeepSeekR1,
when we slightly decrease the activation experts num in inference,
the output quality doesn't change. But the speed of decoding and prefill
is speed up which is inspiring. So our showcase makes use of this finding</em></p>
<h2 id="how-to-run"><a class="header" href="#how-to-run">How to Run</a></h2>
<h3 id="v022--v023-longer-context--fp8-kernel"><a class="header" href="#v022--v023-longer-context--fp8-kernel">v0.2.2 &amp; v0.2.3 longer context &amp; FP8 kernel</a></h3>
<h4 id="longer-context"><a class="header" href="#longer-context">longer context</a></h4>
<p>To use this feature, <a href="https://github.com/flashinfer-ai/flashinfer">install flashinfer</a> first.</p>
<p>Note: The latest MLA kernel in FlashInfer still has a few minor issues. They are continuously fixing them on the main branch. If you are using FlashInfer, please install it from the main source code.</p>
<p>If you want to use long context(longer than 20K) for prefill, enable the matrix absorption MLA during the prefill phase, which will significantly reduce the size of the kv cache. Modify yaml file like this:</p>
<pre><code>- match:
    name: "^model\\.layers\\..*\\.self_attn$"
  replace:
    class: ktransformers.operators.attention.KDeepseekV2Attention # optimized MLA implementation
    kwargs:
      generate_device: "cuda"
      prefill_device: "cuda"
      absorb_for_prefill: True # change this to True to enable long context(prefill may slower).
</code></pre>
<p>If the VRAM is still insufficient, try reducing the <code>chunk_prefill_size</code> parameter (default is 8192) to further decrease the intermediate results during chunk prefill.</p>
<h4 id="fp8-kernel"><a class="header" href="#fp8-kernel">FP8 kernel</a></h4>
<p>The DeepSeek-AI team provides FP8 safetensors for DeepSeek-R1/V3 models. We achieve performance optimization through the following works:</p>
<ul>
<li><strong>FP8 GPU Kernel Integration</strong>: FP8 linear layer acceleration kernels integrated in KTransformers</li>
<li><strong>Hybrid Quantization Architecture</strong>:
<ul>
<li>Attention and Shared-Expert modules use FP8 precision (enhances computational accuracy)</li>
<li>Experts modules retain GGML quantization (GGUF format, reside in CPU to save GPU memory)</li>
</ul>
</li>
</ul>
<p>So those who are persuing the best performance can use the FP8 linear kernel for DeepSeek-V3/R1.</p>
<p>The detailed guide is <a href="en/./fp8_kernel.html">here</a>.</p>
<h3 id="v02--v021-showcase"><a class="header" href="#v02--v021-showcase">V0.2 &amp; V0.2.1 Showcase</a></h3>
<h4 id="single-socket-version-32-cores"><a class="header" href="#single-socket-version-32-cores">Single socket version (32 cores)</a></h4>
<p>Our local_chat test command is:</p>
<pre><code class="language-shell">numactl -N 1 -m 1 python ./ktransformers/local_chat.py --model_path &lt;your model path&gt; --gguf_path &lt;your gguf path&gt;  --prompt_file &lt;your prompt txt file&gt;  --cpu_infer 33 --max_new_tokens 1000
&lt;when you see chat, then press enter to load the text prompt_file&gt;
</code></pre>
<p><code>&lt;your model path&gt;</code> can be local or set from online hugging face like deepseek-ai/DeepSeek-V3. If online encounters connection problem, try use mirror (hf-mirror.com) <br>
<code>&lt;your gguf path&gt;</code> can also be online, but as its large we recommend you download it and quantize the model to what you want (notice it's the dir path) <br>
<code>--max_new_tokens 1000</code> is the max output token length. If you find the answer is truncated, you
can increase the number for longer answer (But be aware of OOM, and increase it will slow down the generation rate.).</p>
<p>The command <code>numactl -N 1 -m 1</code> aims to advoid data transfer between numa nodes<br>
Attention! If you are testing R1 and it may skip thinking. So you can add arg: <code>--force_think true</code>. This is explained in <a href="en/DeepseekR1_V3_tutorial.html#faq">FAQ</a> part</p>
<h4 id="dual-socket-version-64-cores"><a class="header" href="#dual-socket-version-64-cores">Dual socket version (64 cores)</a></h4>
<p>Make sure before you install (use install.sh or <code>make dev_install</code>), setting the env var <code>USE_NUMA=1</code> by <code>export USE_NUMA=1</code> (if already installed, reinstall it with this env var set). You may check the doc <a href="en/./install.html">here</a> for install details. <br></p>
<p>Test Command:</p>
<pre><code class="language-shell"># ---For those who have not installed ktransformers---
# git clone https://github.com/kvcache-ai/ktransformers.git
# cd ktransformers
# git submodule init
# git submodule update
# export USE_NUMA=1
# make dev_install # or sh ./install.sh
# ----------------------------------------------------
python ./ktransformers/local_chat.py --model_path &lt;your model path&gt; --gguf_path &lt;your gguf path&gt;  --prompt_file &lt;your prompt txt file&gt;  --cpu_infer 65 --max_new_tokens 1000
&lt;when you see chat, then press enter to load the text prompt_file&gt;
</code></pre>
<p>The parameters' meaning is the same. But As we use dual socket, we set cpu_infer to 65</p>
<h3 id="v03-showcase"><a class="header" href="#v03-showcase">V0.3 Showcase</a></h3>
<h4 id="dual-socket-version-64-cores-1"><a class="header" href="#dual-socket-version-64-cores-1">Dual socket version (64 cores)</a></h4>
<p>Our local_chat test command is:</p>
<pre><code class="language-shell">wget https://github.com/kvcache-ai/ktransformers/releases/download/v0.1.4/ktransformers-0.3.0rc0+cu126torch26fancy-cp311-cp311-linux_x86_64.whl
pip install ./ktransformers-0.3.0rc0+cu126torch26fancy-cp311-cp311-linux_x86_64.whl
python -m ktransformers.local_chat --model_path &lt;your model path&gt; --gguf_path &lt;your gguf path&gt;  --prompt_file &lt;your prompt txt file&gt;  --cpu_infer 65 --max_new_tokens 1000
&lt;when you see chat, then press enter to load the text prompt_file&gt;
</code></pre>
<p>The parameters' meaning is the same with V0.2. But As we  use dual socket, we set cpu_infer to 65</p>
<h2 id="some-explanations"><a class="header" href="#some-explanations">Some Explanations</a></h2>
<ol>
<li>
<p>Also we want to make further use of our two NUMA nodes on Xeon Gold cpu.
To avoid the cost of data transfer between nodes, we "copy" the critical matrix on
both nodes which takes more memory consumption but accelerates the prefill and decoding process.
But this method takes huge memory and slow when loading weights, So be patient when loading
and monitor the memory usage. We are going to optimize this huge memory overhead. Stay tuned~ <br></p>
</li>
<li>
<p>The command args <code>--cpu_infer 65</code> specifies how many cores to use (it's ok that it exceeds the physical number,
but it's not the more the better. Adjust it slightly lower to your actual number of cores)<br></p>
</li>
<li>
<p>Why CPU/GPU Hybrid Inference?
DeepSeek's MLA operators are highly computationally intensive. While running everything on CPU is possible, offloading the heavy computations to the GPU results in a massive performance boost.</p>
</li>
<li>
<p>Where Does the Speedup Come From?</p>
<ul>
<li>Expert Offload: Unlike traditional layer-based or KVCache offloading (as seen in llama.cpp), we offload the expert computation to the CPU and MLA/KVCache to GPU, aligning perfectly with DeepSeek’s architecture for optimal efficiency.</li>
<li>Intel AMX Optimization – Our AMX-accelerated kernel is meticulously tuned, running several times faster than existing llama.cpp implementations. We plan to open-source this kernel after cleansing and are considering upstream contributions to llama.cpp.</li>
</ul>
</li>
<li>
<p>Why Intel CPUs?
Intel is currently the only CPU vendor that supports AMX-like instructions, which delivers significantly better performance compared to AVX-only alternatives.</p>
</li>
</ol>
<h2 id="next"><a class="header" href="#next">Next</a></h2>
<h3 id="faster"><a class="header" href="#faster">Faster</a></h3>
<ul>
<li>The FlashInfer (https://github.com/flashinfer-ai/flashinfer) project is releasing an even more efficient fused MLA operator, promising further speedups</li>
<li>vLLM has explored multi-token prediction in DeepSeek-V3, and support is on our roadmap for even better performance</li>
<li>We are collaborating with Intel to enhance the AMX kernel (v0.3) and optimize for Xeon6/MRDIMM</li>
</ul>
<h3 id="easier"><a class="header" href="#easier">Easier</a></h3>
<ul>
<li>Official Docker images to simplify installation</li>
<li>Fix the server integration for web API access</li>
<li>Fix the local chat only accepting a single line prompt (currently \n begins generating prompt)</li>
<li>Support for more quantization types, including the highly requested dynamic quantization from unsloth</li>
</ul>
<p>Stay tuned for more updates!</p>
<h2 id="faq"><a class="header" href="#faq">FAQ</a></h2>
<h3 id="r1-no-thinking"><a class="header" href="#r1-no-thinking">R1 No Thinking</a></h3>
<p>Attention! If you are testing R1 and it may skip thinking. So you can add arg: <code>--force_think true</code>. The detail is in <a href="en/./FAQ.html">FAQ</a> part <br></p>
<h3 id="more-faq"><a class="header" href="#more-faq">More FAQ</a></h3>
<p><a href="en/./FAQ.html">See detail</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tutorial-heterogeneous-and-local-moe-inference"><a class="header" href="#tutorial-heterogeneous-and-local-moe-inference">Tutorial: Heterogeneous and Local MoE Inference</a></h1>
<p>DeepSeek-(Code)-V2 is a series of strong mixture-of-experts (MoE) models, featuring a total of 236 billion parameters, with 21 billion parameters activated per token. This model has demonstrated remarkable reasoning capabilities across various benchmarks, positioning it as one of the SOTA open models and nearly comparable in performance to GPT-4. DeepSeek-R1 uses a similar architecture to DeepSeek-V2, but with a bigger number of parameters.</p>
<p align="center">
  <picture>
    <img alt="DeepSeek-Coder-V2 Score" src="en/../assets/BigCodeBench.png" width=80%>
  </picture>
</p>
<p>Moreover, unlike previous models that employed traditional attention mechanisms like Grouped-Query Attention (GQA), DeepSeek-V2 incorporates a novel Multi-head Latent Attention (MLA). This innovation significantly reduces the size of the KV cache required during inference, enhancing efficiency.</p>
<p>However, despite its efficiency, the practicality of running such a large model on personal computing setups seems impractical. Official documentation for DeepSeek-V2 indicates that eight 80GB GPUs are necessary for standard inference operations, and even the scaled-down Q4_k_m version requires at least two 80GB GPUs. These requirements are beyond the reach of most individual researchers and small teams.</p>
<p>Nonetheless, by employing several cutting-edge optimization techniques, we have successfully operated this colossal model on a desktop computer with only 21GB of VRAM and 136GB of DRAM. In this document, we outline the specific optimizations utilized and provide a detailed tutorial on how to implement these strategies using KTransformers.</p>
<h2 id="applied-optimizations"><a class="header" href="#applied-optimizations">Applied Optimizations</a></h2>
<h3 id="optimized-mla-operator"><a class="header" href="#optimized-mla-operator">Optimized MLA Operator</a></h3>
<p>The following figure provides a brief overview of DeepSeek-V2 architecture. At the heart of its attention layer, DeepSeek-V2 introduces a novel MLA operator that represents the heads of key-value pairs using a common, joint compressed representation, which holds significant potential for efficiency improvements. However, the official open-source implementation of the MLA operator explicitly decompresses this compressed representation and caches the decompressed key-value pairs. This process not only enlarges the KV cache size but also diminishes inference performance.</p>
<p align="center">
  <picture>
    <img alt="DeepSeek on KTransformers" src="en/../assets/DeepSeek-on-KTransformers.png" width=80%>
  </picture>
</p>
<p>To truly capitalize on the benefits of MLA, we have implemented an optimized version for inference. According to its original paper, we absorb the decompression matrices directly into the q_proj and out_proj weights. Consequently, the compressed representation does not need to be decompressed to compute the attention. This adjustment significantly reduces the KV cache size and increases the arithmetic intensity of this operator, which greatly optimizes the utilization of GPU computational power.</p>
<h3 id="advanced-quantization-kernels"><a class="header" href="#advanced-quantization-kernels">Advanced Quantization Kernels</a></h3>
<p>The original DeepSeek-V2 model stores its parameters in BF16 format, consuming approximately 470GB of raw storage. This exceeds the RAM capacity available on mainstream desktop computers. To address this, we leverage the well-established GGUF community's quantized weights to simplify the process for users.
However, quantized data types are not typically supported by highly-optimized BLAS packages. As a result, the original HuggingFace Transformers' Torch implementation must dequantize these tensors to supported data types before processing, which introduces unnecessary computational overhead and increases memory traffic. To overcome this, we have incorporated advanced kernels that operate directly on quantized data types, thereby optimizing inference performance.</p>
<p>In the current version of KTransformers, we utilize Marlin for GPU kernels and llamafile for CPU kernels. These kerenls are specially designed to benefit from modern GPU architecture and modern CPU instruction extensions such as AVX512-BF16 (AMD Zen4 or newer) and AVX-VNNI (Intel Alder Lake or newer), that are tailored for quantized data types and machine learning workloads. We also use expert parallelism and other optimization for MOE inferencem on CPU based on llamafile, and call them as CPUInfer.  As demonstrated in Figure 2(cite from Marlin), Marlin can achieve near ideal 3.87x speedup compare to corresponding Torch counterparts. As demonstrated in the following figure, our micro benchmarks show that inference using CPUInfer performs several times faster than Torch in low bits representation. Note that in practical inference such as using transformers, the Torch baseline use BF16 or FP16 as linear weights, and will occupy more memory resources, or it will be more slower due to dequantization when using quanted weights.</p>
<p align="center">
  <picture>
    <img alt="CPUInfer Performance" src="en/../assets/cpuinfer.png" width=80%>
  </picture>
</p>
<p align="center">
  <picture>
    <img alt="marlin performance" src="https://github.com/IST-DASLab/marlin/blob/master/assets/sustained.png?raw=true" width=80%>
  </picture>
</p>
<h3 id="arithmetic-intensity-guided-offloading"><a class="header" href="#arithmetic-intensity-guided-offloading">Arithmetic Intensity Guided Offloading</a></h3>
<p>Storing all 236 billion parameters of a model in GPU VRAM is clearly impractical for local users. Therefore, we strategically store only the most computationally intensive parameters on the GPU. For instance, after our optimizations, the MLA operator, which contains 128 heads with a shared compressed key-value representation, shows an arithmetic intensity of 512. This makes it the most intensive operator, particularly during smaller inference batch sizes. Hence, it is allocated to the GPU to leverage the power of tensor cores.</p>
<p>On the other hand, as shown in Figure 1, each transformer block in DeepSeek-V2 includes 160 mixture-of-experts (MoE) experts, comprising 96% of the total parameters. However, the MoE router activates only 6 out of these 160 experts for each token, which means that only 3.75% of the MoE parameters are utilized during the decoding phase. With a batch size of one, the arithmetic intensity of the MoE operation is roughly 0.075. This operation, primarily involving a batched General Matrix-Vector Multiplication (GEMV), can thus be efficiently handled by the CPU.</p>
<p>Following this principle of arranging all operators by their arithmetic intensity and placing the most intensive ones in the GPU as much as possible, we prioritize positioning the MoE parameters and word embeddings computations on the CPU side to utilize its larger memory capacity. Meanwhile, the remaining parameters, including shared experts, projections in the attention module, and MLA, are stored in the GPU VRAM. As these parameters are accessed by every token, their placement on the GPU maximizes the benefits of high memory bandwidth. This configuration leads to approximately 20.7 GB of VRAM usage and 136GB DRAM memory requests if the Q4_K_M version is used, which is feasible even on a local desktop. Additionally, the placement can be adjusted according to the actual configuration, adhering to the same principle.</p>
<p>Moreover, as an extensible framework, KTransformers is set to support more advanced operators in future releases, continually enhancing its capability to handle diverse workloads efficiently.</p>
<h2 id="yaml-template"><a class="header" href="#yaml-template">YAML Template</a></h2>
<p>To implement the above optimizations in KTransformers, users need to write a YAML file containing the optimized rules.
KTransformers will iterate through all sub-modules of the model, match rules specified in the YAML rule file, and replace them with advanced modules as specified.</p>
<p align="center">
  <picture>
    <img alt="Inject-Struction" src="en/../assets/InjectStruction.png" width=80%>
  </picture>
</p>
<p>Specifically, the following rules are used:</p>
<ul>
<li>Replace the Attention module with our <a href="en/deepseek-v2-injection.html#mla">optimized MLA Operator</a>.</li>
<li>Replace routed experts with <a href="en/deepseek-v2-injection.html#experts">CPUInfer kernels</a> that use Llamafile.</li>
<li>Replace all Linear modules not belonging to attention with <a href="en/deepseek-v2-injection.html#linear">Marlin</a> kernels.</li>
</ul>
<h3 id="mla"><a class="header" href="#mla">MLA</a></h3>
<p>For attention module injection, we only need to match the module name used in Transformers using a regular expression and replace it with our pre-implemented module.
The YAML rule is listed below.</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.self_attn$" # regular expression
  replace:
    class: ktransformers.operators.attention.KDeepseekV2Attention # optimized MLA implementation
</code></pre>
<p>As we can see, each rule in the YAML file has two parts: <code>match</code> and <code>replace</code>.
The match part specifies which module should be replaced, and the replace part specifies the module to be injected into the model along with the initialization keywords.</p>
<h3 id="experts"><a class="header" href="#experts">Routed Experts </a></h3>
<p>For routed experts, the module we inject is a wrapper of CPUInfer, KTransformersExperts. There are several implementations within a wrapper, and we need to specify keywords to tell the wrapper which implementation we want to use and how we intend to use it.</p>
<p>In KTransformers, some models exhibit different behaviors during prefilling and generation for better performance. KTransformersExperts is one of them. All these special modules have a <code>device</code> keyword describing which device the module should be initialized on. Other keywords specify the behaviors during prefilling and generation and may be differ when using different injection modules. Here, we specify which implementation on which device we want to use during prefilling and generation, and which device the output should be on.
Note that we only use these parameters when layer-wise prefilling is enabled; otherwise, prefilling is conducted with the same configuration as generation.</p>
<p>In the original implementation of Transformers, MoE is implemented using <code>nn.ModuleList</code>. We don't want KTransformers to iterate through all the sub-modules in the list, so we set <code>recursive: False</code> in this rule to prevent recursive injection into submodules of the current module. Here is the YAML rule:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert parallelism
    device: "cpu"   # device to load this module on initialization
    kwargs:
      prefill_device: "cuda"
      prefill_op: "KExpertsTorch"
      generate_device: "cpu"
      generate_op:  "KExpertsCPU"
      out_device: "cuda"
  recursive: False # don't recursively inject submodules of this module
</code></pre>
<p>If we inject the expert list as a custom module, we can't use the interface in <code>nn.ModuleList</code> as default. We need to change the forward function in the FFN module. The simplest way is implementing a new module using custom forward function and inject it. We have implemented the new module, and the injection can be done by simply adding an injection rule. We can use the <code>class</code> instead of <code>name</code> to match a module that will be replaced. Here is the YAML rule:</p>
<pre><code class="language-yaml">- match:
    class: ktransformers.models.modeling_deepseek.DeepseekV2MoE
  replace:
    class: ktransformers.operators.experts.KDeepseekV2MoE     # MLP module with custom forward function
</code></pre>
<h3 id="linear"><a class="header" href="#linear">Other Linear Modules</a></h3>
<p>For the remained linear modules, we want to use our quantization kernels. However, we don't want to inject linear in the MLA operator because we currently don't know the effect of using quantization in MLA.
So, we can change our regular expression and add a class check in the match part of the rule. Only modules matching both name and class simultaneously will be injected.
We also need to transfer some keywords similar to the injection of experts. Here is the YAML rule:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\.(?!.*self_attn).*$"  # regular expression 
    class: torch.nn.Linear  # only match modules matching name and class simultaneously
  replace:
    class: ktransformers.operators.linear.KTransformersLinear  # optimized Kernel on quantized data types
    kwargs:
      generate_device: "cuda"
      prefill_device: "cuda"
      generate_op: "KLinearMarlin"
      prefill_op: "KLinearTorch"
</code></pre>
<h3 id="Pre-compute Buffers"><a class="header" href="#Pre-compute Buffers">Pre-compute Buffers </a></h3>
<p>The original model is initialized on the meta device. The rotary embedding module pre-computes some buffers when initializing, which has no effect and doesn't compute anything when using the meta device. Therefore, we need to compute the buffers when loading the model. For convenience, we inject the rotary embedding module with our custom module, which performs pre-computations when loading. Here is the YAML rule:</p>
<pre><code class="language-yaml">- match:
    class: ktransformers.models.modeling_deepseek.DeepseekV2YarnRotaryEmbedding
  replace:
    class: ktransformers.operators.RoPE.YarnRotaryEmbedding
</code></pre>
<h2 id="wrap-your-custom-module"><a class="header" href="#wrap-your-custom-module">Wrap Your Custom Module</a></h2>
<p>We have implemented some modules, but you may need to inject your custom module using KTransformers.
The only thing you need to do is wrap your custom module and write YAML files. We provide a base operator specifying interfaces an injection module should have. You only need to inherit from that module and change the <code>__init__</code>, <code>forward</code>, or <code>load</code> function as needed.</p>
<ul>
<li>The <code>__init__</code> function of the base operator maintains the necessary information for injection and execution of the KTransformers framework. To override this function, subclass modules need to call the base operator's <code>__init__</code> function in their own initializer.</li>
<li>The <code>forward</code> function is a function in torch that will be called during inference, where the module author has the freedom to achieve higher performance.</li>
<li>The <code>load</code> function is used to load all parameters of this module. The default implementation is to call the <code>load</code> function of all submodules. You can modify this function to customize its loading method and explicitly control the loading of its submodules.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tutorial-inject-operator-step-by-step"><a class="header" href="#tutorial-inject-operator-step-by-step">Tutorial: Inject Operator Step by Step</a></h1>
<blockquote>
<p>Author: Azure-Tang</p>
</blockquote>
<h2 id="tldr"><a class="header" href="#tldr">TL;DR</a></h2>
<p>This tutorial will guide you through the process of injecting custom operators into a model using the KTransformers framework. We will use the DeepSeekV2-Chat model as an example to demonstrate how to inject custom operators into the model step by step. The tutorial will cover the following topics:</p>
<ul>
<li><a href="en/injection_tutorial.html#how-to-write-injection-rules">How to write injection rules</a>
<ul>
<li><a href="en/injection_tutorial.html#understanding-model-structure">Understanding the structure of the model</a></li>
</ul>
</li>
<li><a href="en/injection_tutorial.html#muti-gpu">Multi-GPU</a></li>
<li><a href="en/injection_tutorial.html#how-to-write-a-new-operator-and-inject-into-the-model">How to write a new operator and inject it into the model</a></li>
</ul>
<h2 id="how-to-write-injection-rules"><a class="header" href="#how-to-write-injection-rules">How to Write Injection Rules</a></h2>
<p>The basic form of the injection rules for the Inject framework is as follows:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.*$"  # Target module name
    class: torch.nn.Linear  # Target module
  replace:
    class: "default"
    kwargs:
      generate_device: "cuda:0"
      # your_op_param_1: 1234
      # your_op_param_2: 5678
  recursive: True
</code></pre>
<ul>
<li>match: This field marks the matching rules, which can appear in two forms, name and class. These two matching rules can appear together or separately; they only match when both criteria are met.</li>
<li>replace:
<ul>
<li>class: Python class that can be imported to replace the target module. If no replacement is desired, set to default.</li>
<li>kwargs: List of parameters needed for module initialization.
<ul>
<li>generate_device: The device for this module, can be set to “cpu”, “cuda”, “cuda:1”, etc.</li>
</ul>
</li>
</ul>
</li>
<li>recursive: Whether to recursively inject this module’s submodules, default is True.</li>
</ul>
<p>For the recursive field: Some modules contain multiple submodules, such as the Self-attention module typically includes q/k/v/o four linear modules. If we replace the self-attention module but do not want the internal linear modules to be covered by other rules, set this rule to False.</p>
<h2 id="understanding-model-structure"><a class="header" href="#understanding-model-structure">Understanding Model Structure</a></h2>
<p>Using <a href="https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat">deepseek-ai/DeepSeek-V2-Lite-Chat</a> as an example, we can follow the above rules step by step to inject our custom module and run it. KTransformers offers a high degree of flexibility, allowing you to replace/experiment with basic operators. However, it also requires users to clearly understand the structure of the model they are running.</p>
<p>Fortunately, knowing the structure of a model is very simple. Open the file list on the <a href="https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat/tree/main">deepseek-ai/DeepSeek-V2-Lite</a> homepage, and you can see the following files:</p>
<p align="center">
  <picture>
    <img alt="Inject-Struction" src="en/../assets/model_structure_guild.png" width=60%>
  </picture>
</p>
<p>From the <code>.saftensors</code> file, we can see the name of each layer’s weights, corresponding to the match.name attribute in the injection rules.
From the <code>modeling_deepseek.py</code> file, we can see the specific implementation of each module class, with the class name corresponding to the match.class attribute in the injection rules.</p>
<p>The structure of the DeepSeekV2 model from the <code>.saftensors</code> and <code>modeling_deepseek.py</code> files is as follows:</p>
<p align="center">
  <picture>
    <img alt="Inject-Struction" src="en/../assets/deepseekv2_structure.png" width=60%>
  </picture>
</p>
<p>Supported operators and their corresponding classes are as follows:</p>
<div class="table-wrapper"><table><thead><tr><th>match</th><th>replace</th><th>backends</th><th>descriptions</th></tr></thead><tbody>
<tr><td>Linear</td><td>KTransformersLinear</td><td>KLinearMarlin</td><td>Marlin as backend</td></tr>
<tr><td></td><td></td><td>KLinearTorch</td><td>pytorch as backend</td></tr>
<tr><td></td><td></td><td>KLinearCPUInfer</td><td>llamafile as backend</td></tr>
<tr><td></td><td></td><td>KLinearFP8</td><td>Triton fp8_gemm kernel. Requires GPU be able to caluculate fp8 data</td></tr>
<tr><td>experts</td><td>KTransformersExperts</td><td>KExpertsTorch</td><td>pytorch as backend</td></tr>
<tr><td></td><td></td><td>KExpertsMarlin</td><td>Marlin as backend</td></tr>
<tr><td></td><td></td><td>KExpertsCPU</td><td>llamafile as backend</td></tr>
<tr><td>Attention</td><td>KDeepseekV2Attention</td><td>KDeepseekV2Attention</td><td>MLA implementation</td></tr>
<tr><td>MoE</td><td>KMistralSparseMoEBlock</td><td>KQwen2MoeSparseMoeBlock</td><td>MoE for Qwen2</td></tr>
<tr><td></td><td>KDeepseekV2MoE</td><td>KDeepseekV2MoE</td><td>MoE for DeepseekV2</td></tr>
<tr><td>Model</td><td>KQwen2MoeModel</td><td>KQwen2MoeModel</td><td>Model for Qwen2</td></tr>
<tr><td></td><td>KDeepseekV2Model</td><td>KDeepseekV2Model</td><td>Model for DeepseekV2</td></tr>
<tr><td>RoPE</td><td>RotaryEmbedding</td><td>RotaryEmbedding</td><td>RoPE module</td></tr>
<tr><td></td><td>YarnRotaryEmbedding</td><td>YarnRotaryEmbedding</td><td>RoPE module</td></tr>
</tbody></table>
</div>
<p>Then we start step-by-step injection of custom modules, our targets are:</p>
<ul>
<li>Replace the linear module with custom Marlin linear module.</li>
<li>Replace the self-attention module with a custom Absorption-based MLA module.</li>
<li>Replace the experts module with a custom Experts module.</li>
<li>Replace the MoE module with a custom MoE module.</li>
<li>Replace the RoPE module with a custom RoPE module.</li>
<li>Set the running device for each module.</li>
</ul>
<p>The full implementation of the injection rules can be found in the <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat.yaml">here</a>.</p>
<h2 id="matrix-absorption-based-mla-injection"><a class="header" href="#matrix-absorption-based-mla-injection">Matrix Absorption-based MLA Injection</a></h2>
<p>For the injection of the Attention module, we only need to use a regular expression to match the module names used in transformers and replace them with our own MLA module implementation. The YAML injection rule is as follows:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.self_attn$"  # Regular expression
  replace:
    class: ktransformers.operators.attention.KDeepseekV2Attention # Optimized MLA implementation
</code></pre>
<p>As you can see, each rule in the YAML file has two parts: match and replace. The match part specifies the module to be replaced, and the replace part specifies the module to be injected into the model along with the initialization keywords.</p>
<h2 id="injection-of-routed-experts"><a class="header" href="#injection-of-routed-experts">Injection of Routed Experts</a></h2>
<p>For Routed Experts (corresponding to the exps in the diagram), the module we inject is CPUInfer, which is wrapped in the wrapper module KTransformersExperts. KTransformersExperts has multiple implementations, and we need to specify keywords to tell the wrapper module which implementation we want to use and how we plan to use it.</p>
<p>In the source code of the transformer, MoE is implemented using nn.ModuleList. We do not want KTransformers to traverse all submodules in the list and inject them one by one, so in this rule, we set recursive: False to prevent recursive injection into the submodules of this module. The YAML rule is as follows:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # Custom MoE kernel with expert parallelism
    kwargs:
      generate_device: "cpu"
      generate_op: "MLPCPUExperts"
      out_device: "cuda"
  recursive: False # Don't recursively inject submodules of this module
</code></pre>
<p>If we inject Routed Experts as a custom module, we cannot use the interfaces in the original <code>nn.ModuleList</code>. Therefore, it is necessary to modify the forward function in the FFN module. The simplest method is to implement a new module with a custom forward function and inject it.</p>
<pre><code class="language-yaml">- match:
    class: ktransformers.models.modeling_deepseek.DeepseekV2MoE
  replace:
    class: ktransformers.operators.experts.KDeepseekV2MoE     # MLP module with custom forward function
</code></pre>
<h2 id="injection-of-linear-layers"><a class="header" href="#injection-of-linear-layers">Injection of Linear Layers</a></h2>
<p>For the remaining linear layer modules, we aim to use quantized operators to save storage space while improving performance. Since there is no current research on using MLA and quantization together, we do not want to inject linear into the MLA operator. Therefore, we can modify the regular expression and add a type check in the match part of the rule. Only modules that match both the name and class simultaneously will be injected. We also need to pass some keywords similar to the injection of Routed Experts. The YAML rule is as follows:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\.(?!.*self_attn).*$"  # Regular expression
    class: torch.nn.Linear  # Only match modules matching name and class simultaneously
  replace:
    class: ktransformers.operators.linear.KTransformersLinear  # Optimized kernel on quantized data types
    kwargs:
      generate_device: "cuda"
      generate_op: "QuantizedLinearMarlin"
</code></pre>
<h2 id="injection-of-modules-with-pre-calculated-buffers"><a class="header" href="#injection-of-modules-with-pre-calculated-buffers">Injection of Modules with Pre-calculated Buffers</a></h2>
<p>To avoid occupying resources when initializing the injected original model, we use torch’s meta device to initialize the original model. The RoPE module pre-calculates some buffers during initialization, but no calculations are performed when using the meta device. Therefore, we need to compensate for the calculation of the buffer when loading the model. Simply, we inject a custom module into the rotary embedding module, which performs pre-calculation during loading. The YAML rule is as follows:</p>
<pre><code class="language-yaml">- match:
    class: ktransformers.models.modeling_deepseek.DeepseekV2YarnRotaryEmbedding
  replace:
    class: ktransformers.operators.RoPE.YarnRotaryEmbedding
</code></pre>
<h2 id="specifying-running-devices-for-modules"><a class="header" href="#specifying-running-devices-for-modules">Specifying Running Devices for Modules</a></h2>
<p>Finally, we set a fallback basic attribute generate_device for all modules:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.|^lm_head"
  replace:
    class: "default"
    kwargs:
      generate_device: "cuda"
  
- match:
    name: "^model.embed_tokens"
  replace:
    class: "default"
    kwargs:
        generate_device: "cpu"
</code></pre>
<p>Through these two rules, we place all previously unmatched layers (and their submodules) and lm_head on cuda, and the embedding on cpu. Note that the properties of a module will be determined by the first rule it matches. For example, if you later set a new replace.kwargs.generate_device in an injected module, the device set earlier will take precedence. If your computer has multiple cards, you can also configure the model to multiple cards.</p>
<h2 id="muti-gpu"><a class="header" href="#muti-gpu">Muti-GPU</a></h2>
<p>If you have multiple GPUs, you can set the device for each module to different GPUs.
DeepseekV2-Chat got 60 layers, if we got 2 GPUs, we can allocate 30 layers to each GPU. Complete multi GPU rule examples <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml">here</a>.</p>
<p align="center">
  <picture>
    <img alt="Inject-Struction" src="en/../assets/multi_gpu.png" width=60%>
  </picture>
</p>
<p>First of all, for multi-GPU, we have to inject an new operator <code>KDeepseekV2Model</code>. And set division of the layers to different GPUs. For our case, we have to set the <code>transfer_map</code> in the <code>KDeepseekV2Model</code> operatoras as follows:</p>
<pre><code class="language-yaml">- match:
    name: "^model$"
  replace:
    class: "ktransformers.operators.models.KDeepseekV2Model"
    kwargs:
      transfer_map: 
        30: "cuda:1"
</code></pre>
<p>And we have to set the device for each module in the model.</p>
<p>For example, for <code>routed experts</code>, the yaml for one GPU is:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # Custom MoE kernel with expert parallelism
    kwargs:
      generate_device: "cuda:0"
      generate_op: "MLPCUDAExperts"
      out_device: "cuda:0"
  recursive: False # Don't recursively inject submodules of this module
</code></pre>
<p>But for two GPUs, we need to set the device for each module in the model.</p>
<pre><code class="language-yaml"># allcate 0-29 layers‘s out_device to cuda:0
- match:
    name: "^model\\.layers\\.(0|[1-9]|[12][0-9])\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert paralleism
    kwargs:
      generate_device: "cpu"
      generate_op:  "KExpertsCPU"
      out_device: "cuda:0"
  recursive: False # don't recursively inject submodules of this module

# allocate 30-59 layers‘s out_device to cuda:1
- match:
    name: "^model\\.layers\\.([345][0-9])\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert paralleism
    kwargs:
      generate_device: "cpu"
      generate_op:  "KExpertsCPU"
      out_device: "cuda:1"
  recursive: False # don't recursively inject submodules of this module
</code></pre>
<p>For other modules, we can set the device in the same way.</p>
<h2 id="how-to-write-a-new-operator-and-inject-into-the-model"><a class="header" href="#how-to-write-a-new-operator-and-inject-into-the-model">How to Write a New Operator and Inject into the Model</a></h2>
<p>In this section, we will explain how to write an operator that can be injected, using the implementation of a new linear as an example.</p>
<p>First, all injectable operators need to inherit from the BaseInjectedModule class, which inherits some attributes required by our injection framework. Its initialization function needs to meet the following basic format:</p>
<pre><code class="language-python">class LinearTorchInject(BaseInjectedModule):
    def __init__(
        self,
        key: str,
        gguf_loader: GGUFLoader,
        config: PretrainedConfig,
        orig_module: nn.Module = None,
        generate_device: str = "cuda",
        **kwargs,
    ):
        super().__init__(key, gguf_loader, config, orig_module, generate_device, **kwargs)
</code></pre>
<p>If users have other parameters that need to be passed to this class, they can also be included in the init function and re-passed in the kwargs parameter in the yaml file. For example, if our operator wants to pass a parameter <code>my_param</code>, the init function can be written as:</p>
<pre><code class="language-python">class LinearTorchInject(BaseInjectedModule):
    def __init__(
        self,
        key: str,
        gguf_loader: GGUFLoader,
        config: PretrainedConfig,
        orig_module: nn.Module = None,
        generate_device: str = "cuda",
        my_param: bool = True,
        **kwargs,
    ):
        super().__init__(key, gguf_loader, config, orig_module, generate_device, **kwargs)
        self.my_param = my_param
</code></pre>
<p>Then our injection rule can be written as:</p>
<pre><code class="language-yaml">- match: 
    name: "^model\\.layers\\..*$"  # Regular expression matches the module name.
    class: torch.nn.Linear  # Type restrictions can be added.
  replace:
    class: ktransformers.operators.linear.LinearTorchInject  # Inject module path
    kwargs: # Extra parameters
      generate_device: "cuda"
      my_param: True
</code></pre>
<p>For the linear module, it is also necessary to read weights from a gguf file. We provide the <code>KLinearBase</code> class to help users read weights from gguf files. Users only need to inherit and implement the load, unload, and forward functions. Therefore, a fully injectable linear class would look like this:</p>
<pre><code class="language-python">class LinearTorchInject(BaseInjectedModule, KLinearBase):
    def __init__(
        self,
        key: str,
        gguf_loader: GGUFLoader,
        config: PretrainedConfig,
        orig_module: nn.Module = None,
        generate_device: str = "cuda",
        **kwargs,
    ):
        super().__init__(key, gguf_loader, config, orig_module, generate_device, **kwargs)
        KLinearBase.__init__(self)
        self.has_bias = False
        self.dtype = torch.get_default_dtype()
        self.w = None
        self.has_bias = False
    
    def load(self, w: dict | nn.Parameter | tuple | None = None, device: str|None = None):
        if device is None: device = self.device
        if w is None: w = self.load_weight(device=device)

        if isinstance(w, nn.Parameter):
            self.w = w.to(dtype=self.dtype).view(self.out_features, self.in_features).T
            self.has_bias = False
        elif isinstance(w, tuple):
            self.w = w[0].to(dtype=self.dtype).view(self.out_features, self.in_features).T
            self.bias = w[1].to(dtype=self.dtype)
            self.has_bias = True
        else:
            raise ValueError("Invalid weight type")
        self.w = self.w.to(device)
        if self.has_bias:
            self.bias = self.bias.to(device)

    def unload(self):
        if self.w is not None:
            self.w = None
        if self.has_bias:
            self.bias = None

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        dtype = x.dtype
        out_device = x.device
        x = x.to(device=self.device, dtype=self.dtype)
        x = x @ self.w
        if self.has_bias:
            x = x + self.bias
        x = x.to(dtype=dtype, device=out_device)
        return x
</code></pre>
<p>Note that the <code>self.load_weight</code> function is provided by the KLinearBase class to help users load weights from a gguf file into the module. The implementation details of KLinearBase can be found on <a href="https://github.com/kvcache-ai/ktransformers/blob/44f57270c9514d79fab224186d90ccf61059331a/ktransformers/operators/linear.py#L31">GITHUB</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="muti-gpu-1"><a class="header" href="#muti-gpu-1">Muti-GPU</a></h1>
<p>Assume you have read the <a href="en/./injection_tutorial.html">Injection Tutorial</a> and have a basic understanding of how to inject a model. In this tutorial, we will show you how to use KTransformers to run a model on multiple GPUs.</p>
<p>If you have multiple GPUs, you can set the device for each module to different GPUs.
DeepseekV2-Chat got 60 layers, if we got 2 GPUs, we can allocate 30 layers to each GPU. Complete multi GPU rule examples <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml">here</a>.</p>
<p align="center">
  <picture>
    <img alt="Inject-Struction" src="en/../assets/multi_gpu.png" width=60%>
  </picture>
</p>
<p>First of all, for multi-GPU, we have to inject an new operator <code>KDeepseekV2Model</code>. And set division of the layers to different GPUs. For our case, we have to set the <code>transfer_map</code> in the <code>KDeepseekV2Model</code> operatoras as follows:</p>
<pre><code class="language-yaml">- match:
    name: "^model$"
  replace:
    class: "ktransformers.operators.models.KDeepseekV2Model"
    kwargs:
      transfer_map: 
        30: "cuda:1"
</code></pre>
<p>And we have to set the device for each module in the model.</p>
<p>For example, for <code>routed experts</code>, the yaml for one GPU is:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # Custom MoE kernel with expert parallelism
    kwargs:
      generate_device: "cuda:0"
      generate_op: "MLPCUDAExperts"
      out_device: "cuda:0"
  recursive: False # Don't recursively inject submodules of this module
</code></pre>
<p>But for two GPUs, we need to set the device for each module in the model.</p>
<pre><code class="language-yaml"># allcate 0-29 layers‘s out_device to cuda:0
- match:
    name: "^model\\.layers\\.(0|[1-9]|[12][0-9])\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert paralleism
    kwargs:
      generate_device: "cpu"
      generate_op:  "KExpertsCPU"
      out_device: "cuda:0"
  recursive: False # don't recursively inject submodules of this module

# allocate 30-59 layers‘s out_device to cuda:1
- match:
    name: "^model\\.layers\\.([345][0-9])\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert paralleism
    kwargs:
      generate_device: "cpu"
      generate_op:  "KExpertsCPU"
      out_device: "cuda:1"
  recursive: False # don't recursively inject submodules of this module
</code></pre>
<p>For other modules, we can set the device in the same way.</p>
<h1 id="how-to-fully-utilize-multi-gpus-vram"><a class="header" href="#how-to-fully-utilize-multi-gpus-vram">How to fully utilize multi-GPU's VRAM</a></h1>
<p>When you have multiple GPUs, you can fully utilize the VRAM of each GPU by moving more weights to the GPU.</p>
<p>For example, for DeepSeekV2-Chat, we can move the weights of the experts to the GPU.</p>
<p>For example, the yaml for two GPUs is:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\.(0|[1-9]|[12][0-9])\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts
    kwargs:
      generate_device: "cpu"
      generate_op:  "KExpertsCPU"
      out_device: "cuda:0"
  recursive: False
</code></pre>
<p>But we got extra 60GB VRAM on cuda:0, we can move experts in layer 4~8 to cuda:0.</p>
<pre><code class="language-yaml"># Add new rule before old rule.
- match:
    name: "^model\\.layers\\.([4-8])\\.mlp\\.experts$" # inject experts in layer 4~8 as marlin expert
  replace:
    class: ktransformers.operators.experts.KTransformersExperts  
    kwargs:
      generate_device: "cuda:0"
      generate_op:  "KExpertsMarlin"
  recursive: False

- match:
    name: "^model\\.layers\\.(0|[1-9]|[12][0-9])\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     
    kwargs:
      generate_device: "cpu"
      generate_op:  "KExpertsCPU"
      out_device: "cuda:0"
  recursive: False 
</code></pre>
<p>Adjust the layer range as you want. Note that:</p>
<ul>
<li>The loading speed will be significantly slower for each expert moved to the GPU.</li>
<li>You have to close the cuda graph if you want to move the experts to the GPU.</li>
<li>For DeepSeek-R1/V3, each expert moved to the GPU will consume approximately 6GB of VRAM.</li>
<li>The first matched rule in yaml will be applied. For example, if you have two rules that match the same layer, only the first rule's replacement will be valid.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fp8-linear-kernel-for-deepseek-v3r1"><a class="header" href="#fp8-linear-kernel-for-deepseek-v3r1">FP8 Linear Kernel for DeepSeek-V3/R1</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>The DeepSeek-AI team provides FP8 safetensors for DeepSeek-R1/V3 models. We achieve performance optimization through the following works:</p>
<ul>
<li><strong>FP8 GPU Kernel Integration</strong>: FP8 linear layer acceleration kernels integrated in KTransformers</li>
<li><strong>Hybrid Quantization Architecture</strong>:
<ul>
<li>Attention and Shared-Expert modules use FP8 precision (enhances computational accuracy)</li>
<li>Experts modules retain GGML quantization (GGUF format, reside in CPU to save GPU memory)</li>
</ul>
</li>
</ul>
<p>So those who are persuing the best performance can use the FP8 linear kernel for DeepSeek-V3/R1.</p>
<h2 id="key-features"><a class="header" href="#key-features">Key Features</a></h2>
<p>✅ Hybrid Precision Architecture (FP8 + GGML)<br>
✅ Memory Optimization (~19GB VRAM usage)</p>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<h3 id="using-pre-merged-weights"><a class="header" href="#using-pre-merged-weights">Using Pre-Merged Weights</a></h3>
<p>Pre-merged weights are available on Hugging Face:<br>
<a href="https://huggingface.co/KVCache-ai/DeepSeek-V3">KVCache-ai/DeepSeek-V3-GGML-FP8-Hybrid</a><br>
<a href="https://huggingface.co/KVCache-ai/DeepSeek-R1">KVCache-ai/DeepSeek-R1-GGML-FP8-Hybrid</a></p>
<blockquote>
<p>Please confirm the weights are fully uploaded before downloading. The large file size may extend Hugging Face upload time.</p>
</blockquote>
<p>Download Pre-Merged Weights</p>
<pre><code class="language-shell">pip install -U huggingface_hub

# Optional: Use HF Mirror for faster downloads in special area.
# export HF_ENDPOINT=https://hf-mirror.com 

huggingface-cli download --resume-download KVCache-ai/DeepSeek-V3-GGML-FP8-Hybrid --local-dir &lt;local_dir&gt;
</code></pre>
<h3 id="using-merge-scripts"><a class="header" href="#using-merge-scripts">Using merge scripts</a></h3>
<p>If you got local DeepSeek-R1/V3 fp8 safetensors and gguf weights(eg.q4km), you can merge them using the following scripts.</p>
<pre><code class="language-shell">python merge_tensors/merge_safetensor_gguf.py \
  --safetensor_path &lt;fp8_safetensor_path&gt; \
  --gguf_path &lt;gguf_folder_path&gt; \
  --output_path &lt;merged_output_path&gt;
</code></pre>
<ul>
<li><code>--safetensor_path</code>:	input path of safetensor file(<a href="https://huggingface.co/deepseek-ai/DeepSeek-V3/tree/main">Download</a>).</li>
<li><code>--gguf_path</code>: input path of gguf folder (<a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M">Download</a>).</li>
<li><code>--output_path</code>: output path of merged file.</li>
</ul>
<h3 id="execution-notes"><a class="header" href="#execution-notes">Execution Notes</a></h3>
<p>Launch local_chat.py with custom quantized experts</p>
<pre><code class="language-shell">python ktransformers/local_chat.py \
  --model_path deepseek-ai/DeepSeek-V3 \
  --gguf_path &lt;merged_weights_folder&gt; \
  --optimize_config_path ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts.yaml \
  --cpu_infer &lt;cpu_cores + 1&gt;
</code></pre>
<h2 id="notes"><a class="header" href="#notes">Notes</a></h2>
<p>⚠️ Hardware Requirements<br></p>
<ul>
<li>Recommended minimum 19GB available VRAM for FP8 kernel.</li>
<li>Requires GPU with FP8 support (e.g., 4090)</li>
</ul>
<p>⏳ First-Run Optimization
JIT compilation causes longer initial execution (subsequent runs retain optimized speed).</p>
<p>🔄 Temporary Interface<br>
Current weight loading implementation is provisional - will be refined in future versions</p>
<p>📁 Path Specification<br>
Despite hybrid quantization, merged weights are stored as .safetensors - pass the containing folder path to <code>--gguf_path</code></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rocm-support-for-ktransformers-beta"><a class="header" href="#rocm-support-for-ktransformers-beta">ROCm Support for ktransformers (Beta)</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<h3 id="overview-1"><a class="header" href="#overview-1">Overview</a></h3>
<p>In our effort to expand GPU architecture support beyond NVIDIA, we are excited to introduce <strong>AMD GPU support through ROCm</strong> in ktransformers (Beta release). This implementation has been tested and developed using EPYC 9274F processors and AMD Radeon 7900xtx GPUs.</p>
<h2 id="installation-guide"><a class="header" href="#installation-guide">Installation Guide</a></h2>
<h3 id="1-install-rocm-driver"><a class="header" href="#1-install-rocm-driver">1. Install ROCm Driver</a></h3>
<p>Begin by installing the ROCm drivers for your AMD GPU:</p>
<ul>
<li><a href="https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html">Official ROCm Installation Guide for Radeon GPUs</a></li>
</ul>
<h3 id="2-set-up-conda-environment"><a class="header" href="#2-set-up-conda-environment">2. Set Up Conda Environment</a></h3>
<p>We recommend using Miniconda3/Anaconda3 for environment management:</p>
<pre><code class="language-bash"># Download Miniconda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

# Create environment
conda create --name ktransformers python=3.11
conda activate ktransformers

# Install required libraries
conda install -c conda-forge libstdcxx-ng

# Verify GLIBCXX version (should include 3.4.32)
strings ~/anaconda3/envs/ktransformers/lib/libstdc++.so.6 | grep GLIBCXX
</code></pre>
<blockquote>
<p><strong>Note:</strong> Adjust the Anaconda path if your installation directory differs from <code>~/anaconda3</code></p>
</blockquote>
<h3 id="3-install-pytorch-for-rocm"><a class="header" href="#3-install-pytorch-for-rocm">3. Install PyTorch for ROCm</a></h3>
<p>Install PyTorch with ROCm 6.2.4 support:</p>
<pre><code class="language-bash">pip3 install torch torchvision torchaudio \
  --index-url https://download.pytorch.org/whl/rocm6.2.4
pip3 install packaging ninja cpufeature numpy
</code></pre>
<blockquote>
<p><strong>Tip:</strong> For other ROCm versions, visit <a href="https://pytorch.org/get-started/previous-versions/">PyTorch Previous Versions</a></p>
</blockquote>
<h3 id="4-build-ktransformers"><a class="header" href="#4-build-ktransformers">4. Build ktransformers</a></h3>
<pre><code class="language-bash"># Clone repository
git clone https://github.com/kvcache-ai/ktransformers.git
cd ktransformers
git submodule update --init

# Optional: Compile web interface
# See: api/server/website.md

# Install dependencies
bash install.sh
</code></pre>
<h2 id="running-deepseek-r1-models"><a class="header" href="#running-deepseek-r1-models">Running DeepSeek-R1 Models</a></h2>
<h3 id="configuration-for-24gb-vram-gpus"><a class="header" href="#configuration-for-24gb-vram-gpus">Configuration for 24GB VRAM GPUs</a></h3>
<p>Use our optimized configuration for constrained VRAM:</p>
<pre><code class="language-bash">python ktransformers/local_chat.py \
  --model_path deepseek-ai/DeepSeek-R1 \
  --gguf_path &lt;path_to_gguf_files&gt; \
  --optimize_config_path ktransformers/optimize/optimize_rules/rocm/DeepSeek-V3-Chat.yaml \
  --cpu_infer &lt;cpu_cores + 1&gt;
</code></pre>
<blockquote>
<p><strong>Beta Note:</strong> Current Q8 linear implementation (Marlin alternative) shows suboptimal performance. Expect optimizations in future releases.</p>
</blockquote>
<h3 id="configuration-for-40gb-vram-gpus"><a class="header" href="#configuration-for-40gb-vram-gpus">Configuration for 40GB+ VRAM GPUs</a></h3>
<p>For better performance on high-VRAM GPUs:</p>
<ol>
<li>
<p>Modify <code>DeepSeek-V3-Chat.yaml</code>:</p>
<pre><code class="language-yaml"># Replace all instances of:
KLinearMarlin → KLinearTorch
</code></pre>
</li>
<li>
<p>Execute with:</p>
<pre><code class="language-bash">python ktransformers/local_chat.py \
  --model_path deepseek-ai/DeepSeek-R1 \
  --gguf_path &lt;path_to_gguf_files&gt; \
  --optimize_config_path &lt;modified_yaml_path&gt; \
  --cpu_infer &lt;cpu_cores + 1&gt;
</code></pre>
</li>
</ol>
<blockquote>
<p><strong>Tip:</strong> If you got 2 * 24GB AMD GPUS, you may also do the same modify and run <code>ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml</code> instead.</p>
</blockquote>
<h2 id="known-limitations"><a class="header" href="#known-limitations">Known Limitations</a></h2>
<ul>
<li>Marlin operations not supported on ROCm platform</li>
<li>Current Q8 linear implementation shows reduced performance (Beta limitation)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backend-services-server"><a class="header" href="#backend-services-server">Backend Services (Server)</a></h1>
<p>The Server offers fast heterogeneous inference capabilities of ktransformers through an API for external usage.</p>
<img src="en/api/server/server-arch.png" height="600" alt="Server architecture">
<h2 id="api"><a class="header" href="#api">API</a></h2>
<p>The Server provides model inference services externally through a RESTful API, with two methods of interaction: ChatCompletion and Assistant.</p>
<ul>
<li>The ChatCompletion interface requires users to provide all historical dialogues at once, after which the model responds. AI service providers (such as <a href="https://platform.openai.com/docs/api-reference/chat/create">OpenAI</a>) and local inference frameworks (such as <a href="https://github.com/ollama/ollama/blob/main/docs/api.md">Ollama</a>) both offer the ChatCompletion interface. To ensure compatibility with OpenAI and Ollama, the Server offers APIs that are consistent with theirs. Therefore, applications currently using OpenAI and Ollama can seamlessly switch to our Server. For example: <a href="en/api/server/tabby.html">How to use Tabby and ktransformers locally with a 236B model for code completion?</a>.</li>
<li>The Assistant is suitable for applications that need to reuse a series of resources and call the model. For instance, in educational applications, developers can create an Assistant named "Second Grade Math Teacher" and set an initial prompt ("You are an experienced second-grade math teacher..."), and upload relevant materials (second grade math textbooks). After creating the Assistant, the application needs to create a Thread to store the dialogues between the user and the model (Message). When calling the model, the application creates a Run to obtain the Assistant's response. Compared to ChatCompletion, the Assistant-enabled Server handles the reuse of conversational contexts and multi-turn dialogues, making model calls in complex scenarios more convenient. The <a href="https://platform.openai.com/docs/api-reference/assistants/createAssistant">OpenAI Assistant API</a> introduces such an Assistant interface, and the Server provides a consistent API.</li>
</ul>
<p>These API definitions are located in <code>server/api</code>, and their specific usage can be seen <a href="en/api/server/api.html">here</a>.</p>
<h2 id="integrating-model-inference-frameworks"><a class="header" href="#integrating-model-inference-frameworks">Integrating Model Inference Frameworks</a></h2>
<p>The Server uses ktransformers for model calling and inference. It also supports other inference frameworks, such as the already supported <a href="https://huggingface.co/docs/transformers/index">transformers</a>, and plans to support <a href="https://github.com/turboderp/exllamav2">exllamav2</a>. These functionalities are implemented in <code>server/backend</code>.</p>
<p>The model inference functionalities of the frameworks are abstracted into a base class <code>BackendInterfaceBase</code>. This class includes a function: inference. It takes historical dialogue information messages as input and returns the text result from the model. The inference function adopts an async generator design, allowing the Server to return model responses in a streaming manner.</p>
<pre><code class="language-python">class BackendInterfaceBase:
  async def inference(self, messages, **kwargs)-&gt;AsyncIterator[str]:
    ...
</code></pre>
<p>This inference function naturally implements the functionality of ChatCompletion because its inputs and outputs are historical dialogues and model responses, respectively. Thus, the ChatCompletion API can directly call the inference function to complete model inference.</p>
<p>Assistant is more complex than ChatCompletion, requiring the Server to store the related state of the Assistant and call the inference function appropriately. The Server maintains a set of Assistant logic in the database, storing the Assistants, Threads, and Messages created by applications. In memory, the Server maintains a <code>ThreadContext</code> for each Thread, gathering information related to each Thread's Assistant, etc. When a user sends a new Message, the Server calls the get_local_messages function of ThreadContext to obtain messages and then calls the inference function to get the inference results.</p>
<pre><code class="language-python">class MyThreadContext(ThreadContext):
    def get_local_messages(self):
      ...
</code></pre>
<p>Since different model inference frameworks have different historical dialogue input formats, <code>ThreadContext</code> and <code>BackendInterface</code> need to be used in pairs. Besides its own ktransformers, the Server also supports transformers. For integrating other model inference frameworks, refer to the implementations of <code>TransformersInterface</code> and <code>TransformersThreadContext</code> in <a href="https://github.com/kvcache-ai/ktransformers-dev/blob/main/ktransformers/server/backend/interfaces/transformers.py">transformers.py</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="start-with-website"><a class="header" href="#start-with-website">Start with website</a></h1>
<p>This document provides the necessary steps to set up and run the web service for this project.</p>
<h2 id="1-starting-the-web-service"><a class="header" href="#1-starting-the-web-service">1. Starting the Web Service</a></h2>
<h3 id="11-compiling-the-web-code"><a class="header" href="#11-compiling-the-web-code">1.1. Compiling the Web Code</a></h3>
<p>Before you can compile the web code, make sure you have installed <a href="https://nodejs.org">Node.js</a> version 18.3 or higher</p>
<p>Note: The version of Node.js in the Ubuntu or Debian GNU/Linux software repository is too low, causing compilation errors. Users can also install Node.js through the Nodesource repository, provided they uninstall the outdated version first.</p>
<pre><code class="language-bash">
  # sudo apt-get remove nodejs npm -y &amp;&amp; sudo apt-get autoremove -y
  sudo apt-get update -y &amp;&amp; sudo apt-get install -y apt-transport-https ca-certificates curl gnupg
  curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /usr/share/keyrings/nodesource.gpg
  sudo chmod 644 /usr/share/keyrings/nodesource.gpg
  echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_23.x nodistro main" | sudo tee /etc/apt/sources.list.d/nodesource.list
  sudo apt-get update -y
  sudo apt-get install nodejs -y

</code></pre>
<p>Once npm is installed, navigate to the <code>ktransformers/website</code> directory:</p>
<pre><code class="language-bash">cd ktransformers/website
</code></pre>
<p>Next, install the Vue CLI with the following command:</p>
<pre><code class="language-bash">npm install @vue/cli
</code></pre>
<p>Now you can build the project:</p>
<pre><code class="language-bash">npm run build
</code></pre>
<p>Finally you can build ktransformers with website:</p>
<pre><code>cd ../../
pip install .
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-to-use-tabby-and-ktransformers-locally-with-236b-large-models-for-code-completion"><a class="header" href="#how-to-use-tabby-and-ktransformers-locally-with-236b-large-models-for-code-completion">How to Use Tabby and ktransformers Locally with 236B Large Models for Code Completion?</a></h1>
<p><a href="https://tabby.tabbyml.com/docs/welcome/">Tabby</a> is an open-source code assistant that allows users to manually configure the backend framework and model, and use it across multiple IDEs/editors, such as VSCode and IntelliJ. Since Tabby can interface with Ollama on the framework side, and the ktransformers server provides a consistent API with Ollama, we can connect Tabby to the ktransformers server. This setup allows us to experience fast, heterogeneous inference in code completion scenarios.</p>
<ol>
<li>Start ktransformers.</li>
</ol>
<pre><code class="language-bash">./ktransformers --port 9112
</code></pre>
<ol start="2">
<li>Install Tabby: Follow the official tutorial to install Tabby on a Linux server or Windows PC with an NVIDIA GPU <a href="https://tabby.tabbyml.com/docs/quick-start/installation/linux/">here</a>.</li>
<li>Configure Tabby: Create <code>~/.tabby/config.toml</code> and add the following configuration.</li>
</ol>
<pre><code class="language-toml">[model.completion.http]
kind = "ollama/completion"
api_endpoint = "http://127.0.0.1:9112/"
model_name = "DeepSeek-Coder-V2-Instruct"
prompt_template = "&lt;｜fim▁begin｜&gt;{prefix}&lt;｜fim▁hole｜&gt;{suffix}&lt;｜fim▁end｜&gt;" # Prompt Template
</code></pre>
<p>In this configuration, <code>kind</code> specifies that ktransformers uses the standard Ollama API to serve Tabby; <code>api_endpoint</code> matches the interface bound when launching ktransformers; <code>model_name</code> is set to the model used by ktransformers, here <code>DeepSeek-Coder-V2-Instruct</code> is the backend inference model; <code>prompt_template</code> is the model's prompt template, which requires a corresponding template for different models to use the Fill In the Middle feature properly.
Here we demonstrate the relevant configuration for Tabby using the Ollama API to provide the Completion feature. For configuration information about other functions available in Tabby, refer to <a href="https://tabby.tabbyml.com/docs/administration/model/">here</a>.</p>
<ol start="4">
<li>
<p>Start the Tabby service: <code>./tabby serve</code>.
<img src="en/api/server/run-tabby.png" alt="image-20240709112329577" style="zoom:50%;" /></p>
<p>After launching, you should see access to the <code>/api/tags</code> interface in the ktransformers command line (in version v0.13.0 of Tabby, this changes to access to the <code>/api/show/</code> interface).
<img src="en/api/server/visit-api-tags.png" alt="image-20240709111648215" style="zoom:67%;" /></p>
</li>
<li>
<p>Register a Tabby account, obtain a Token: After starting the Tabby service, open the corresponding link in a browser (as shown above at 0.0.0.0:8080), and follow the <a href="https://tabby.tabbyml.com/docs/quick-start/register-account/">tutorial</a> to create a user and get a Token.</p>
</li>
<li>
<p>Start VSCode, install the Tabby extension plugin, and use the Token obtained in the previous step to connect to the Tabby Server, following <a href="https://tabby.tabbyml.com/docs/extensions/installation/vscode/">here</a>.</p>
</li>
<li>
<p>Open any code file and experience the fast heterogeneous inference of ktransformers.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="makefile"><a class="header" href="#makefile">Makefile</a></h1>
<h2 id="target"><a class="header" href="#target">Target</a></h2>
<h3 id="flake_find"><a class="header" href="#flake_find">flake_find:</a></h3>
<pre><code class="language-bash">make flake_find
</code></pre>
<p>find all the python files under ./ktransformers dir and find the Error, Warning, Fatal... (their codes) into a list that are not consistent with the pep8 standard. For now we have get all this list in the .flake8 file's extend-ignore section in order to let flakes8 ignore them temporarily.(we may improve them in the future)</p>
<h3 id="format"><a class="header" href="#format">format:</a></h3>
<pre><code class="language-bash">make format
</code></pre>
<p>we use black to format all the python files under ./ktransformers dir. It obeys the pep8 standard
but we modify the line length to 120 by add</p>
<pre><code class="language-toml">[tool.black]
line-length = 120
preview = true
unstable = true
</code></pre>
<p>in the pyproject.toml file.</p>
<h3 id="dev_install"><a class="header" href="#dev_install">dev_install:</a></h3>
<pre><code class="language-bash">make dev_install
</code></pre>
<p>install the package in the development mode. It means that the package is installed in the editable mode. So if you modify the code, you don't need to reinstall the package. We recommend the developer to use this method to install the package.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- omit in toc -->
<h1 id="faq-1"><a class="header" href="#faq-1">FAQ</a></h1>
<ul>
<li><a href="en/FAQ.html#install">Install</a>
<ul>
<li><a href="en/FAQ.html#q-importerror-libx86_64-linux-gnulibstdcso6-version-glibcxx_3432-not-found">Q: ImportError: /lib/x86_64-linux-gnu/libstdc++.so.6: version GLIBCXX_3.4.32' not found</a></li>
<li><a href="en/FAQ.html#q-deepseek-r1-not-outputting-initial--token">Q: DeepSeek-R1 not outputting initial  token</a></li>
</ul>
</li>
<li><a href="en/FAQ.html#usage">Usage</a>
<ul>
<li><a href="en/FAQ.html#q-if-i-got-more-vram-than-the-models-requirement-how-can-i-fully-utilize-it">Q: If I got more VRAM than the model's requirement, how can I fully utilize it?</a></li>
<li><a href="en/FAQ.html#q-if-i-dont-have-enough-vram-but-i-have-multiple-gpus-how-can-i-utilize-them">Q: If I don't have enough VRAM, but I have multiple GPUs, how can I utilize them?</a></li>
<li><a href="en/FAQ.html#q-how-to-get-the-best-performance">Q: How to get the best performance?</a></li>
<li><a href="en/FAQ.html#q-my-deepseek-r1-model-is-not-thinking">Q: My DeepSeek-R1 model is not thinking.</a></li>
<li><a href="en/FAQ.html#q-loading-gguf-error">Q: Loading gguf error</a></li>
<li><a href="en/FAQ.html#q-version-glibcxx_3430-not-found">Q: Version `GLIBCXX_3.4.30' not found</a></li>
<li><a href="en/FAQ.html#q-when-running-the-bfloat16-moe-model-the-data-shows-nan">Q: When running the bfloat16 moe model, the data shows NaN</a></li>
<li><a href="en/FAQ.html#q-using-fp8-prefill-very-slow">Q: Using fp8 prefill very slow.</a></li>
<li><a href="en/FAQ.html#q-possible-ways-to-run-graphics-cards-using-volta-and-turing-architectures">Q: Possible ways to run graphics cards using volta and turing architectures</a></li>
</ul>
</li>
</ul>
<h2 id="install"><a class="header" href="#install">Install</a></h2>
<h3 id="q-importerror-libx86_64-linux-gnulibstdcso6-version-glibcxx_3432-not-found"><a class="header" href="#q-importerror-libx86_64-linux-gnulibstdcso6-version-glibcxx_3432-not-found">Q: ImportError: /lib/x86_64-linux-gnu/libstdc++.so.6: version GLIBCXX_3.4.32' not found</a></h3>
<pre><code>in Ubuntu 22.04 installation need to add the:
sudo add-apt-repository ppa:ubuntu-toolchain-r/test
sudo apt-get update
sudo apt-get install --only-upgrade libstdc++6
</code></pre>
<p>from-https://github.com/kvcache-ai/ktransformers/issues/117#issuecomment-2647542979</p>
<h3 id="q-deepseek-r1-not-outputting-initial--token"><a class="header" href="#q-deepseek-r1-not-outputting-initial--token">Q: DeepSeek-R1 not outputting initial <think> token</a></h3>
<blockquote>
<p>from deepseek-R1 doc:<br>
Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting "&lt;think&gt;\n\n&lt;/think&gt;") when responding to certain queries, which can adversely affect the model's performance. To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with "&lt;think&gt;\n" at the beginning of every output.</p>
</blockquote>
<p>So we fix this by manually adding "&lt;think&gt;\n" token at prompt end (you can check out at local_chat.py),
and pass the arg <code>--force_think true </code> can let the local_chat initiate the response with "&lt;think&gt;\n"</p>
<p>from-https://github.com/kvcache-ai/ktransformers/issues/129#issue-2842799552</p>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<h3 id="q-if-i-got-more-vram-than-the-models-requirement-how-can-i-fully-utilize-it"><a class="header" href="#q-if-i-got-more-vram-than-the-models-requirement-how-can-i-fully-utilize-it">Q: If I got more VRAM than the model's requirement, how can I fully utilize it?</a></h3>
<ol>
<li>
<p>Get larger context.</p>
<ol>
<li>local_chat.py: You can increase the context window size by setting <code>--max_new_tokens</code> to a larger value.</li>
<li>server: Increase the `--cache_lens' to a larger value.</li>
</ol>
</li>
<li>
<p>Move more weights to the GPU.
Refer to the ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-4.yaml</p>
<pre><code class="language-yaml">- match:
   name: "^model\\.layers\\.([4-10])\\.mlp\\.experts$" # inject experts in layer 4~10 as marlin expert
 replace:
   class: ktransformers.operators.experts.KTransformersExperts  
   kwargs:
     generate_device: "cuda:0" # run in cuda:0; marlin only support GPU
     generate_op:  "KExpertsMarlin" # use marlin expert
 recursive: False
</code></pre>
<p>You can modify layer as you want, eg. <code>name: "^model\\.layers\\.([4-10])\\.mlp\\.experts$"</code> to <code>name: "^model\\.layers\\.([4-12])\\.mlp\\.experts$"</code> to move more weights to the GPU.</p>
<blockquote>
<p>Note: The first matched rule in yaml will be applied. For example, if you have two rules that match the same layer, only the first rule's replacement will be valid.
Note：Currently, executing experts on the GPU will conflict with CUDA Graph. Without CUDA Graph, there will be a significant slowdown. Therefore, unless you have a substantial amount of VRAM (placing a single layer of experts for DeepSeek-V3/R1 on the GPU requires at least 5.6GB of VRAM), we do not recommend enabling this feature. We are actively working on optimization.
Note KExpertsTorch is untested.</p>
</blockquote>
</li>
</ol>
<h3 id="q-if-i-dont-have-enough-vram-but-i-have-multiple-gpus-how-can-i-utilize-them"><a class="header" href="#q-if-i-dont-have-enough-vram-but-i-have-multiple-gpus-how-can-i-utilize-them">Q: If I don't have enough VRAM, but I have multiple GPUs, how can I utilize them?</a></h3>
<p>Use the <code>--optimize_config_path ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml</code> to load the two optimized rule yaml file. You may also use it as an example to write your own 4/8 gpu optimized rule yaml file.</p>
<blockquote>
<p>Note: The ktransformers' multi-gpu stratigy is pipline, which is not able to speed up the model's inference. It's only for the model's weight distribution.</p>
</blockquote>
<h3 id="q-how-to-get-the-best-performance"><a class="header" href="#q-how-to-get-the-best-performance">Q: How to get the best performance?</a></h3>
<p>You have to set <code>--cpu_infer</code> to the number of cores you want to use. The more cores you use, the faster the model will run. But it's not the more the better. Adjust it slightly lower to your actual number of cores.</p>
<h3 id="q-my-deepseek-r1-model-is-not-thinking"><a class="header" href="#q-my-deepseek-r1-model-is-not-thinking">Q: My DeepSeek-R1 model is not thinking.</a></h3>
<p>According to DeepSeek, you need to enforce the model to initiate its response with "&lt;think&gt;\n" at the beginning of every output by passing the arg <code>--force_think True </code>.</p>
<h3 id="q-loading-gguf-error"><a class="header" href="#q-loading-gguf-error">Q: Loading gguf error</a></h3>
<p>Make sure you:</p>
<ol>
<li>Have the <code>gguf</code> file in the <code>--gguf_path</code> directory.</li>
<li>The directory only contains gguf files from one model. If you have multiple models, you need to separate them into different directories.</li>
<li>The folder name it self should not end with <code>.gguf</code>, eg. <code>Deep-gguf</code> is correct, <code>Deep.gguf</code> is wrong.</li>
<li>The file itself is not corrupted; you can verify this by checking that the sha256sum matches the one from huggingface, modelscope, or hf-mirror.</li>
</ol>
<h3 id="q-version-glibcxx_3430-not-found"><a class="header" href="#q-version-glibcxx_3430-not-found">Q: Version `GLIBCXX_3.4.30' not found</a></h3>
<p>The detailed error:</p>
<blockquote>
<p>ImportError: /mnt/data/miniconda3/envs/xxx/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/xxx/xxx/ktransformers/./cpuinfer_ext.cpython-312-x86_64-linux-gnu.so)</p>
</blockquote>
<p>Running <code>conda install -c conda-forge libstdcxx-ng</code> can solve the problem.</p>
<h3 id="q-when-running-the-bfloat16-moe-model-the-data-shows-nan"><a class="header" href="#q-when-running-the-bfloat16-moe-model-the-data-shows-nan">Q: When running the bfloat16 moe model, the data shows NaN</a></h3>
<p>The detailed error:</p>
<pre><code class="language-shell">Traceback (most recent call last):
  File "/root/ktransformers/ktransformers/local_chat.py", line 183, in &lt;module&gt;
    fire.Fire(local_chat)
  File "/usr/local/lib/python3.10/dist-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/usr/local/lib/python3.10/dist-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/usr/local/lib/python3.10/dist-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/root/ktransformers/ktransformers/local_chat.py", line 177, in local_chat
    generated = prefill_and_generate(
  File "/root/ktransformers/ktransformers/util/utils.py", line 204, in prefill_and_generate
    next_token = decode_one_tokens(cuda_graph_runner, next_token.unsqueeze(0), position_ids, cache_position, past_key_values, use_cuda_graph).to(torch_device)
  File "/root/ktransformers/ktransformers/util/utils.py", line 128, in decode_one_tokens
    next_token = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element &lt; 0
</code></pre>
<p><strong>SOLUTION</strong>: The issue of running ktransformers on Ubuntu 22.04 is caused by the current system's g++ version being too old, and the pre-defined macros do not include avx_bf16. We have tested and confirmed that it works on g++ 11.4 in Ubuntu 22.04.</p>
<h3 id="q-using-fp8-prefill-very-slow"><a class="header" href="#q-using-fp8-prefill-very-slow">Q: Using fp8 prefill very slow.</a></h3>
<p>The FP8 kernel is build by JIT, so the first run will be slow. The subsequent runs will be faster.</p>
<h3 id="q-possible-ways-to-run-graphics-cards-using-volta-and-turing-architectures"><a class="header" href="#q-possible-ways-to-run-graphics-cards-using-volta-and-turing-architectures">Q: Possible ways to run graphics cards using volta and turing architectures</a></h3>
<p>From: https://github.com/kvcache-ai/ktransformers/issues/374</p>
<ol>
<li>First, download the latest source code using git.</li>
<li>Then, modify the DeepSeek-V3-Chat-multi-gpu-4.yaml in the source code and all related yaml files, replacing all instances of KLinearMarlin with KLinearTorch.</li>
<li>Next, you need to compile from the ktransformers source code until it successfully compiles on your local machine.</li>
<li>Then, install flash-attn. It won't be used, but not installing it will cause an error.</li>
<li>Then, modify local_chat.py, replacing all instances of flash_attention_2 with eager.</li>
<li>Then, run local_chat.py. Be sure to follow the official tutorial's commands and adjust according to your local machine's parameters.</li>
<li>During the running process, check the memory usage. Observe its invocation through the top command. The memory capacity on a single CPU must be greater than the complete size of the model. (For multiple CPUs, it's just a copy.)
Finally, confirm that the model is fully loaded into memory and specific weight layers are fully loaded into the GPU memory. Then, try to input content in the chat interface and observe if there are any errors.</li>
</ol>
<p>Attention, for better perfomance, you can check this <a href="https://github.com/kvcache-ai/ktransformers/issues/374#issuecomment-2667520838">method</a> in the issue</p>
<blockquote>
<p>https://github.com/kvcache-ai/ktransformers/blob/89f8218a2ab7ff82fa54dbfe30df741c574317fc/ktransformers/operators/attention.py#L274-L279</p>
<pre><code class="language-diff">+ original_dtype = query_states.dtype
+ target_dtype = torch.half
+ query_states = query_states.to(target_dtype)
+ compressed_kv_with_k_pe = compressed_kv_with_k_pe.to(target_dtype)
+ compressed_kv = compressed_kv.to(target_dtype)
+ attn_output = attn_output.to(target_dtype)

decode_attention_fwd_grouped(query_states, compressed_kv_with_k_pe, compressed_kv, attn_output,
                            page_table,
                            position_ids.squeeze(0).to(torch.int32)+1, attn_logits,
                            4, #num_kv_splits # follow vLLM, fix it TODO
                            self.softmax_scale,
                            past_key_value.page_size)

+ attn_output = attn_output.to(original_dtype)
</code></pre>
<p>https://github.com/kvcache-ai/ktransformers/blob/89f8218a2ab7ff82fa54dbfe30df741c574317fc/ktransformers/operators/attention.py#L320-L326</p>
<pre><code class="language-diff">- attn_output = flash_attn_func( 
-     query_states, 
-     key_states, 
-     value_states_padded, 
-     softmax_scale=self.softmax_scale, 
-     causal=True, 
- )
+ attn_output = F.scaled_dot_product_attention(
+     query_states.transpose(1, 2),
+     key_states.transpose(1, 2),
+     value_states_padded.transpose(1, 2),
+     scale=self.softmax_scale,
+     is_causal=True
+ ).transpose(1, 2)
</code></pre>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h2 id="hello-everyone-here-is-the-successfully-reproduced-environment-configuration-for-your-reference"><a class="header" href="#hello-everyone-here-is-the-successfully-reproduced-environment-configuration-for-your-reference">Hello everyone, here is the successfully reproduced environment configuration for your reference:</a></h2>
<h3 id="case-1"><a class="header" href="#case-1">Case 1</a></h3>
<ul>
<li>Configuration: l40s 48G + 9654 x2 (192 cores) + 768G DDR5 12-channel</li>
<li>Performance: prefill 108 tokens/s, decode 10.8 tokens/s</li>
<li>Used version: main source code compiled</li>
</ul>
<h3 id="case-2"><a class="header" href="#case-2">Case 2</a></h3>
<ul>
<li>Configuration: Dual Xeon 6430 32C processors, totaling 64 cores and 128 threads, 480GB DDR5 memory, single 4090 24G graphics card</li>
<li>Performance: Running speed approximately 6-8 tokens per second</li>
</ul>
<h2 id="note"><a class="header" href="#note">NOTE</a></h2>
<p>If there are any other configurations that have been successfully run, please feel free to let us know. We will keep updating for everyone to refer to when reproducing. (It has been found that it also works on 2080, AMD, etc. (doge : )
<a href="https://docs.qq.com/smartsheet/form/AVxgQOYhhNfl%2FBB08J2%2Fv3rnnq?tab=BB08J2">click here</a></p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="benchmark"><a class="header" href="#benchmark">Benchmark</a></h2>
<p>To conduct a quick and convenient check, we have employed a simple Python script available <a href="https://github.com/kvcache-ai/ktransformers/tree/main/ktransformers/tests">here</a> to assess the precision of our <strong><a href="https://github.com/kvcache-ai/ktransformers">ktransformers</a></strong> project. For this evaluation, we utilized the same dataset, which was shuffled in a consistent manner and limited to the first 1,000 data points, to test our implementation across a variety of CPU kernels, MLA kernels, and quantization formats.</p>
<p>We selected the DeepSeek-V3 model in its bf16, int8, and q4km versions for this test. The MMLU dataset, which can be found <a href="https://huggingface.co/datasets/cais/mmlu">here</a>, was used (we selected all datasets and shuffled them with a fixed random seed).</p>
<p><strong>!!! However, we skipped the few-shot part and only chose the first 1,000 data points for a quick check.</strong> Please note that this approach may result in results that are not consistent with the technical report of DeepSeek-V3. And the test of R1 and further more tests are on going.</p>
<p>To verify our results, we chose <a href="https://cloud.siliconflow.cn/models">cloud service platform</a> as baseline. All tests were conducted using the same script and datasets, allowing us to make a preliminary assessment of our project's precision.</p>
<p>We set the argument <code>temperature=0.6</code>, and to simplify the test process, we skipped the few-shot part and used the following prompt: <code>There is a single choice question. Answer the question by replying A, B, C, D. No other answers are accepted. Just the letter. \nQuestion: {question}\nA. {option_a}\nB. {option_b}\nC. {option_c}\nD. {option_d}\nAnswer: '</code>. For more details, please refer to the <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/tests/mmlu_test.py">script</a>.</p>
<p>Given that we have only tested 1,000 cases, which provides only a preliminary judgment, some fluctuations in the results are reasonable. We selected all datasets and shuffled them with a fixed random seed to ensure consistency.</p>
<h2 id="some-details"><a class="header" href="#some-details">Some Details</a></h2>
<ul>
<li>
<p>The bf16 model of DeepSeek-V3 is available <a href="https://huggingface.co/opensourcerelease/DeepSeek-V3-bf16/tree/main">here</a> (you may convert it to gguf by llama.cpp). The q4km model can be found <a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M">here</a>.</p>
</li>
<li>
<p>The optimization YAML file is located <a href="https://github.com/kvcache-ai/ktransformers/tree/main/ktransformers/optimize/optimize_rules">here</a>. For the GEMM Kernel, you can change <code>KLinearMarlin</code> to <code>KLinearTorch</code>.</p>
</li>
<li>
<p>To switch the MLA Kernel from Triton to Torch, you can check and modify <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/operators/attention.py">this file</a>, specifically by using the <code>forward_windows</code> method.</p>
</li>
<li>
<p>When attempting to conduct the bf16 test (both CPU Weight and GPU Weight), you may encounter issues stemming from older versions of g++ and as, particularly when using Ubuntu 20 or earlier versions. To facilitate a smoother experience and enable you to reproduce our results, we have provided a development container. This container offers a pre-configured environment tailored for this purpose. However, please note that the container does not have the ktrans package installed. Therefore, you may still need to manually install certain packages to ensure everything runs smoothly.</p>
<ul>
<li>You may config the model mount dir in <code>devcontainer/devcontainer.json</code>, check the <code>"mouts":</code> config.</li>
</ul>
</li>
</ul>
<h2 id="the-result-table"><a class="header" href="#the-result-table">The Result Table</a></h2>
<p>Uses DeepSeek-V3 model (Some specific cases are R1)</p>
<div class="table-wrapper"><table><thead><tr><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>DataSet</td><td>CPU Weight Format</td><td>CPU Kernel</td><td>GPU Weight Format</td><td>GEMM Kernel</td><td>MLA Kernel</td><td><a href="https://cloud.siliconflow.cn/models">Siliconflow</a><br></td><td>Ktrans Point</td></tr>
<tr><td>MMLU<br><br>(shuffle 1k)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>1</td><td>bf16</td><td>cpuinfer</td><td>bf16</td><td>torch</td><td>torch</td><td>81.6</td><td>81.9</td></tr>
<tr><td>2</td><td>q8_0</td><td>cpuinfer</td><td>bf16</td><td>torch</td><td>torch</td><td>81.6</td><td>83.1</td></tr>
<tr><td>3</td><td>q4km</td><td>cpuinfer</td><td>bf16</td><td>torch</td><td>triton</td><td>81.6</td><td>81.4</td></tr>
<tr><td>4</td><td>q4km</td><td>cpuinfer</td><td>q4km-&gt;marlin 8</td><td>marlin</td><td>triton</td><td>81.6</td><td>81.1</td></tr>
<tr><td>5</td><td>q4km</td><td>cpuinfer</td><td>q4km-&gt;marlin 4</td><td>marlin</td><td>triton</td><td>81.6</td><td>81</td></tr>
<tr><td>6</td><td>q4km</td><td>cpuinfer</td><td>fp8</td><td>fp8gemm</td><td>triton</td><td>81.6</td><td>81.5</td></tr>
<tr><td>7 (DeepSeek-R1)</td><td>iq1</td><td>cpuinfer</td><td>fp8</td><td>fp8gemm</td><td>triton</td><td>78.6</td><td>83.6</td></tr>
<tr><td>MMLU-pro<br>(shuffle 1k)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>1</td><td>q4km</td><td>cpuinfer</td><td>fp8</td><td>fp8gemm</td><td>triton</td><td>57.7</td><td>57.6</td></tr>
<tr><td>2</td><td>q4km</td><td>cpuinfer</td><td>q4km-&gt;marlin 4</td><td>marlin</td><td>triton</td><td>57.7</td><td>57.5</td></tr>
<tr><td>3 (DeepSeek-R1)</td><td>iq1</td><td>cpuinfer</td><td>fp8</td><td>fp8gem</td><td>triton</td><td>71.9</td><td>tbd</td></tr>
<tr><td>HumanEval</td><td>tbd</td><td>tbd</td><td>tbd</td><td>tbd</td><td>tbd</td><td>tbd</td><td>tbd</td></tr>
<tr><td>GSM8K</td><td>tbd</td><td>tbd</td><td>tbd</td><td>tbd</td><td>tbd</td><td>tbd</td><td>tbd</td></tr>
</tbody></table>
</div>
<p><strong>The details for each case are listed below</strong>:</p>
<p>By default, The MLA kernel uses triton in linux and torch in windows. But we need to test torch in linux, so we manually modify the <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/operators/attention.py#L592">file</a>. Just get rid of all the if branch and force it to use <code>self.forward_windows</code></p>
<ul>
<li>MMLU test
<ol>
<li><a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml">v3-chat_yaml</a> change all the <code>KLinearMarlin</code> to <code>KLinearTorch</code> (just find all the usage in this file). The source weight comes from <a href="https://huggingface.co/opensourcerelease/DeepSeek-V3-bf16">there</a> (you need to use llama.cpp to convert it to gguf)</li>
<li><a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml">v3-chat_yaml</a>. You need to modify the code to separately load cpu's expert weight. We leave this as comment in these places: <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/operators/experts.py#L122">1</a>, <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/operators/experts.py#L136">2</a>, <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/operators/experts.py#L137">3</a> (note in 3, change the path to your local weight file path). The weight file for q8_0 is <a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q8_0">here</a></li>
<li><a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml">v3-chat_yaml</a>. You need to modify the code to separately load cpu's expert weight. We leave this as comment in these places: <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/operators/experts.py#L122">1</a>, <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/operators/experts.py#L136">2</a>, <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/operators/experts.py#L137">3</a> (note in 3, change the path to your local weight file path). The weight file for q4km is <a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M">here</a></li>
<li><a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml">v3-chat_yaml</a>. You don't need to change the source code as they both use q4km. But note the yaml file <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml#L29">here</a> and <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml#L18">here</a>, below these lines you need to add <code>num_bits: 8</code> (in other words: add this kwargs to all that use <code>KLinearMarlin</code>). The weight file for q4km is <a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M">here</a></li>
<li><a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml">v3-chat_yaml</a>. No need to change yaml, just use the default. The weight file for q4km is <a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M">here</a></li>
<li>You should check the <a href="en/./fp8_kernel.html">doc</a> to learn how to test this case. This is a mixture tensor case.</li>
<li>You should check the <a href="en/./fp8_kernel.html">doc</a> to learn how to test this case. This is a mixture tensor case.</li>
</ol>
</li>
<li>MMLU-pro test
<ol>
<li>You should check the <a href="en/./fp8_kernel.html">doc</a> to learn how to test this case. This is a mixture tensor case.</li>
<li><a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml">v3-chat_yaml</a>. No need to change yaml, just use the default. The weight file for q4km is <a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M">here</a></li>
<li>You should check the <a href="en/./fp8_kernel.html">doc</a> to learn how to test this case. This is a mixture tensor case.</li>
</ol>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>

        <script src="ace.js"></script>
        <script src="editor.js"></script>
        <script src="mode-rust.js"></script>
        <script src="theme-dawn.js"></script>
        <script src="theme-tomorrow_night.js"></script>

        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
